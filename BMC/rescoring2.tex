%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins
%  [twocolumn]

\documentclass[twocolumn]{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
\usepackage{graphicx}
%\usepackage{rotating}
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%address
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\def\includegraphic{}
%\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Correcting the docking pose generation error on binding affinity prediction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
%   corref={aff1},                       % id of corresponding address, if any
%   noteref={n1},                       % id's of article notes, if any
   email={jackyleehongjian@gmail.com}   % email address
]{\inits{HL}\fnm{Hongjian} \snm{Li}}
\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={ksleung@cse.cuhk.edu.hk}
]{\inits{KSL}\fnm{Kwong-Sak} \snm{Leung}}
\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={mhwong@cse.cuhk.edu.hk}
]{\inits{MHW}\fnm{Man-Hon} \snm{Wong}}
\author[
   addressref={aff2,aff3,aff4,aff5},
   corref={aff2,aff3,aff4,aff5},                      % id of corresponding address, if any
   email={E-Mail: pedro.ballester@inserm.fr; Tel.: +33-486-977-265; Fax: +33-486-977-499}
]{\inits{PJB}\fnm{Pedro J.} \snm{Ballester}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{
  \orgname{Department of Computer Science and Engineering, Chinese University of Hong Kong},
  \city{Hong Kong},
  \cny{China}
}
\address[id=aff2]{
  \orgname{Cancer Research Center of Marseille, INSERM U1068},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}
\address[id=aff3]{
  \orgname{Institut Paoli-Calmettes},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}
\address[id=aff4]{
  \orgname{Aix-Marseille Université},
  \postcode{F-13284}
  \city{Marseille},
  \cny{France}
}
\address[id=aff5]{
  \orgname{CNRS UMR7258},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

%\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract}

\parttitle{Background}

Pose generation error is usually measured by comparing the geometry of the pose generated by the docking software and that of the same molecule co-crystallised with the considered protein. Surprisingly, the impact of this error on binding affinity prediction is yet to be systematically analysed across diverse protein-ligand complexes.

\parttitle{Results}

Against commonly-held views, pose generation error has generally a small impact on the accuracy in binding affinity prediction. This is also true for large pose generation errors and it is not only observed with machine-learning scoring functions, but also with classical scoring functions such as AutoDock Vina. Furthermore, we propose a procedure to correct for this error which consists of calibrating the scoring functions with re-docked, rather than co-crystallised, poses. As a result, test set performance after this error-correcting procedure is virtually the same as that of predicting the binding affinity in the absence of pose generation error (i.e. on crystal structures). We evaluated several strategies, obtaining better results for those using a single docking pose per ligand.

\parttitle{Conclusions}

In practice, binding affinity prediction is carried out on the docking pose of a known binder rather than its co-crystallised pose. Our results suggest than pose generation error is in general far less damaging for binding affinity prediction than it is believed. From a practical standpoint, the proposed procedure largely corrects this error. The resulting machine-learning scoring function is freely available at http://istar.cse.cuhk.edu.hk/rf-score-4.tgz and http://crcm.marseille.inserm.fr/fileadmin/rf-score-4.tgz.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{molecular docking}
\kwd{binding affinity}
\kwd{drug discovery}
\kwd{machine learning}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Introduction}

Molecular docking tools are routinely utilised to predict the binding pose as well as the binding affinity of a ligand, usually a small organic molecule, when it is bound to a target protein of interest. The predicted pose helps to reveal the putative intermolecular interactions and therefore understand the mechanism of protein-ligand binding, whereas the predicted affinity reflects the binding strength and therefore prioritizes strong-binding ligands over weak-binding ones from a large library of compounds to evaluate.

A typical docking program implements a sampling algorithm to generate possible binding poses and a scoring function to estimate their binding affinity. The former operation is known as pose generation, and the latter is known as scoring. Modern docking tools such as AutoDock Vina \cite{595} and idock \cite{1362} are already capable of generating near-native poses with a redocking success rate of more than 50\% on three diverse benchmarks \cite{1362}.

Meanwhile, recent years have seen the emergence and prosperity of a new class of scoring functions that use machine learning techniques to increase the accuracy of binding affinity prediction, with RF-Score \cite{564} being the first machine-learning scoring function that introduced a significant improvement over classical, non-machine-learning scoring functions. Since then, numerous enhancements have been applied to the original RF-Score \cite{564} and thereby resulting in RF-Score-v2 \cite{1370} and RF-Score-v3 \cite{1647}, and other relevant studies \cite{1432}. RF-Score has been utilised \cite{1281} to successfully discover a large number of innovative binders of antibacterial DHQase2 targets, demonstrating its practical utility. To promote its use, RF-Score-v3 has been incorporated into a popular web platform called istar \cite{1362}, available at http://istar.cse.cuhk.edu.hk/idock, for large-scale docking. A recent study \cite{1663} has highlighted the benefit of training machine-learning scoring functions with low-quality structural and interaction data.

In prospective virtual screening \cite{1362}, scoring of the docked poses of a molecule is required because the experimentally determined pose is not available in most cases. Therefore, accurate prediction of binding affinity of docked poses, rather than co-crystallised poses, is vital for ranking compounds from a large molecular database. Nevertheless, to the best of our knowledge, all existing machine-learning scoring functions are trained on numerical features derived from crystal poses exclusively, without considering the impact of pose generation error. Hence their applicability to scoring docked poses remains unexamined, and the high accuracy vigorously claimed in their corresponding studies would thus potentially downgrade when their methods are applied to scoring docked poses.

Pose generation error is typically measured by comparing the geometry of the pose generated by the docking software and that of the same molecule co-crystallised with the considered protein (Figure \ref{fig:4WAF}). The impact of this error on binding affinity prediction is yet to be systematically analysed across diverse protein-ligand complexes. In this study we investigate the impact of pose generation error on the predictive performance of both classical and machine-learning scoring functions, and propose a novel approach to correct such error. We also study their capability of predicting near-native poses that are conformationally close to the crystal pose.

\section*{Methods}

This section introduces four scoring functions building upon AutoDock Vina, two benchmarks to evaluate and compare performance of these scoring functions, the performance metrics and our experimental setup.

\subsection*{Model 1 - AutoDock Vina}

AutoDock Vina \cite{595} was chosen as a baseline scoring function because it is considerably popular among the research community. According to Google Scholar, there are more than 2300 citations to date, making Vina one of the most cited docking software. Vina's popularity roots in its substantial improvements on both the average accuracy of the binding mode predictions and the running speed. Its remarkable performance in pose generation as well as its open source nature make it an attractive research tool.

Like all classical scoring functions, Vina assumes a predetermined functional form. Vina's score for the $k$th pose of a molecule is given by the predicted free energy of binding to the target protein and computed as:

\begin{equation}
\label{rfscore4:e_k}
e'_k=\frac{e_{k,inter}+e_{k,intra}-e_{1,intra}}{1+w_6N_{rot}}
\end{equation}

where

\begin{eqnarray}
\label{rfscore4:e_k_inter}
e_{k,inter} &=& w_1 \cdot Gauss1_k \nonumber \\
            &+& w_2 \cdot Gauss2_k \nonumber \\
		    &+& w_3 \cdot Repulsion_k \nonumber \\
		    &+& w_4 \cdot Hydrophobic_k \nonumber \\
		    &+& w_5 \cdot HBonding_k
\end{eqnarray}

\begin{eqnarray}
\label{rfscore4:w}
w_1 &=& -0.035579 \nonumber \\
w_2 &=& -0.005156 \nonumber \\
w_3 &=&  0.840245 \nonumber \\
w_4 &=& -0.035069 \nonumber \\
w_5 &=& -0.587439 \nonumber \\
w_6 &=&  0.05846
\end{eqnarray}

$e'_k$ is the predicted free energy of binding reported by the Vina software when scoring the $k$th docked pose. $e_{k,inter}$ and $e_{k,intra}$ are the inter-molecular and intra-molecular contributions, respectively, which have the same functional form as described in equation \ref{rfscore4:e_k_inter} but sum over different atom pairs. The values for the six weights were calculated by Ordinary Least Squares (OLS) using a nonlinear optimisation algorithm as it has been the case in related force-field scoring functions \cite{1454}, although this process was not disclosed in the original publication \cite{595}. $N_{rot}$ is the calculated number of rotatable bonds. The predicted free energy of binding in kcal/mol units was converted into pKd with $pK_d=-0.73349480509e$ so as to compare to binding affinities in $pK_d$ or $pK_i$ units. Expressions and further details can be found in \cite{1362}.

Unlike in our previous study \cite{1647}, where $k=1$ because only the crystal pose was considered, in this study we aim at training and testing on docked poses, so $k$ can be an arbitrary value, typically ranging from 1 to 9. Thus $e_{k,intra}$ and $e_{1,intra}$ cannot be necessarily cancelled out. As a result, the five terms from $e_{k,intra}$ were not omitted but instead incorporated in models 2, 3 and 4.

\subsection*{Model 2 - MLR::Vina}

This model retains the 11 unweighted Vina terms (5 from $e_{k,inter}$, 5 from $e_{k,intra}$, and $N_{rot}$) as features, but changes the regression method to multiple linear regression (MLR), a regression model commonly adopted by classical scoring functions, such as empirical scoring functions. The use of MLR implies an additive functional form and therefore MLR::Vina is a classical scoring function.

Vina, unlike other classical scoring functions, is not exactly a sum of energetic terms because $w_6\neq0$. In order to make the problem amenable to MLR, we performed a grid search on $w_6$ and thereafter ran MLR on the remaining weights. Specifically, we sampled 101 values for $w_6$ from 0 to 1 with a step size of 0.01. Interesting we found that the $w_6$ values of the best models were always between 0.000 and 0.030. Then we again sampled 31 values for $w_6$ in this range with step size 0.001, and used the one that resulted in the lowest RMSE (Root Mean Square Error) on the training set.

\subsection*{Model 3 - RF::Vina}

This model also retains the 11 unweighted Vina terms as features, but changes the regression method to Random Forest (RF) \cite{1309} so as to implicitly learn the functional form from the data. Hence this model circumvents the modeling assumption of a predetermined functional form and thus allows to investigate the impact of such modelling assumption. Other machine learning techniques such as SVR (Support Vector Regression) \cite{1295} can certainly be applied to this problem, although this is out of the scope of the study.

A RF is an ensemble of a bunch of different decision trees randomly generated from the same training data with bootstraping \cite{1309}. RF trains its constituent trees using the CART algorithm \cite{1310}. RF selects the best data split at each node of the tree from a typically small number (mtry) of randomly chosen features. In regression applications, the RF prediction is given by arithmetic mean of all the individual tree predictions in the forest.

For each value of the mtry control parameter from 1 to all 11 features, we built a RF model with the default number of trees (500). The selected model was the one that led to the lowest RMSE on a subset of training data known as the OOB (Out of Bag) data. Because RF is stochastic, this process was repeated ten times with ten different random seeds. The predictive performance was reported for the RF with the best seed that resulted in the lowest RMSE on the test set. Further details on RF model building in this context can be found in \cite{1647}.

\subsection*{Model 4 - RF::VinaElem}

This models retains RF as the regression method, but expands the feature set to 47 features by adding the 36 RF-Score \cite{564} features. Like in the training process of model 3, the same ten seeds were used, and for a given random seed, a RF model for each mtry value from 1 to 47 was built and that with the lowest RMSE on OOB data was selected. The predictive performance was reported for the RF with the best seed that led to the lowest RMSE on the test set.

To calculate RF-Score features, atom types were selected so as to generate features that are as dense as possible, while considering all the heavy atoms commonly observed in PDB complexes (C, N, O, F, P, S, Cl, Br, I). As the number of protein-ligand contacts is constant for a particular complex, the more atom types are considered, the sparser the resulting features will be. Therefore, we selected a minimal set of atom types by considering atomic number only. Furthermore, a smaller set of interaction features has the additional advantage of leading to computationally faster scoring functions.

RF-Score features are defined as the occurrence count of intermolecular contacts between elemental atom types $i$ and $j$, as shown in equations \ref{rfscore4:x_ij} and \ref{rfscore4:x}, where $d_{kl}$ is the Euclidean distance between the $k$th protein atom of type $j$ and the $l$th ligand atom of type $i$ calculated from a structure; $K_j$ is the total number of protein atoms of type $j$ ($\#\{j\}=9$, considered protein atom types are C, N, O, S) and $L_i$ is the total number of ligand atoms of type $i$ ($\#\{i\}=4$, considered ligand atom types are C, N, O, F, P, S, Cl, Br, I) in the considered complex; $\mathcal{H}$ is the Heaviside step function that counts contacts within a $d_{cutoff}$ neighbourhood. For example, $x_{7,8}$ is the number of occurrences of protein oxygen atoms hypothetically interacting with ligand nitrogen atoms within a chosen neighbourhood. Full details on RF-Score features are available at \cite{564,1295}.

\begin{equation}
\label{rfscore4:x_ij}
x_{ij}=\sum_{k=1}^{K_j}\sum_{l=1}^{L_i}\mathcal{H}(d_{cutoff}-d_{kl})
\end{equation}

\begin{equation}
\label{rfscore4:x}
\mathbf x=\{x_{ij}\}\in N^{36}
\end{equation}

\subsection*{The PDBbind benchmark}

We reused the PDBbind v2007 benchmark because the four models have been evaluated on it in the previous chapter, permitting a direct comparison. Briefly, the test set comprises 195 diverse complexes with measured binding affinities spanning more than 12 orders of magnitude, whereas the training set comprises 1105 non-overlapping complexes.

\subsection*{The 2013 blind benchmark}

We reused the PDBbind v2013 blind benchmark because the four models have been evaluated on it in the previous chapter, allowing a direct comparison. Briefly, the test set comprises 382 complexes newly added in the 2013 release, whereas the training set comprises 2897 complexes from PDBbind v2012 refined set.

\subsection*{Performance measures}

As usual \cite{1313}, performance will be measured by the Standard Deviation (SD), Root Mean Square Error (RMSE), Pearson correlation (Rp) and Spearman rank-correlation (Rs) between predicted and measured binding affinity. SD is included to permit comparison to previously-tested scoring functions on this benchmark. RMSE, on the other hand, reflects the ability of the scoring function to report an accurate binding affinity value. Rs shows how well it can rank bound ligands according to binding strength. Rp simply shows how linear the correlation is and thus it is a less relevant indicator of the quality of the prediction. The mathematical expressions of these four metrics can be found in \cite{1432}.

\begin{equation}
RMSE = \sqrt{\frac{1}{N}\sum_{n=1}^N(p^{(n)}-y^{(n)})^2}
\label{eqn:rmse}
\end{equation}

\begin{equation}
SD = \sqrt{\frac{1}{N-2}\sum_{n=1}^N(\hat{p}^{(n)}-y^{(n)})^2}
\label{eqn:sdev}
\end{equation}

\begin{equation}
R_p = \frac{N\sum_{n=1}^Np^{(n)}y^{(n)}-\sum_{n=1}^Np^{(n)}\sum_{n=1}^Ny^{(n)}}{\sqrt{(N\sum_{n=1}^N(p^{(n)})^2-(\sum_{n=1}^Np^{(n)})^2)(N\sum_{n=1}^N(y^{(n)})^2-(\sum_{n=1}^Ny^{(n)})^2)}}
\label{eqn:pcor}
\end{equation}

\begin{equation}
R_s = \frac{N\sum_{n=1}^Np_r^{(n)}y_r^{(n)}-\sum_{n=1}^Np_r^{(n)}\sum_{n=1}^Ny_r^{(n)}}{\sqrt{(N\sum_{n=1}^N(p_r^{(n)})^2-(\sum_{n=1}^Np_r^{(n)})^2)(N\sum_{n=1}^N(y_r^{(n)})^2-(\sum_{n=1}^Ny_r^{(n)})^2)}}
\label{eqn:scor}
\end{equation}

The Root-Mean Square Deviation (RMSD) quantifies how different the 3D geometry of the redocked pose is from the corresponding co-crystallized pose of the same ligand molecule (i.e. the pose generation error).

%where Na is the number of heavy atoms, r （x ，y ，z is the 3D coordinate of the nth heavy atom of the crystal pose, and （x ，y ，z is the 3D coordinate of the nth heavy atom of the docked pose

\subsection*{Experimental design}

To generate docked poses, each ligand in the two benchmarks was docked into the binding site of its target protein using Vina. This process is known as redocking. As usual \cite{1362}, the search space was defined first by finding the smallest cubic box that covers the entire ligand and then by extending the box in X, Y, Z dimensions by 10\AA. Redocking a ligand resulted in up to nine docked poses output by Vina.

Here we define two schemes to refer to different poses from which the features are extracted. In scheme 1, the chosen pose is the crystal pose. In scheme 2, the chosen pose is the docked pose with the best Vina score, i.e. the one with the lowest Vina score in terms of estimated free energy. We trained the four models on both crystal and docked poses (in both schemes), and tested them also on both crystal and docked poses (in both schemes). Hereafter whenever we mention the docked pose, we implicitly refer to the one with the best Vina score, if not specified explicitly.

In this study, each of the 1300 co-crystallized ligands was redocked into the binding site of its target protein using Vina with default settings. Previously, a script was written to automatically define the search space by finding the smallest cubic box that covers the entire ligand and subsequently extending the box by 10\AA\ in all the three dimensions. For each molecule, Vina returned a maximum number of nine docked poses, of which the one with the best Vina score was used. A second script was written to compute their RMSD with respect to the corresponding co-crystallized pose. Because we aimed at investigating the impact of pose generation error on the prediction of binding affinity, a second test set was defined where each of the 195 complexes has its ligand re-docked and its binding affinity predicted by the scoring functions previously trained on the 1105 crystal structures. As a baseline, these scoring functions were also tested on the co-crystallized ligands of the same 195 complexes. It is noteworthy that, in redocked poses, Vina achieved a relatively small pose generation error in the test set (52\% of the ligands had a docked pose with RMSD < 2\AA). %please explain how waters and ions are dealt with

\section*{Results}

\subsection*{Pose generation error slightly worsens binding affinity prediction}

After redocking by Vina, we used root mean square deviation (RMSD) to quantify the pose generation error, i.e. how different the 3D geometry of the redocked pose is from the corresponding crystal pose of the same ligand molecule. A RMSD value of 2\AA\ was used as a publicly accepted positive control for correct bound structure prediction. 101 out of the 195 ligands (52\%) in the PDBbind v2007 benchmark and 219 out of the 382 ligands (57\%) in the PDBbind v2013 blind benchmark had their best-scoring docked pose with RMSD < 2\AA. When all the docked poses (up to nine) were considered, these redocking success rates increased to 76\% and 81\%, respectively. These results are consistent with those obtained in \cite{1362}, where Vina managed to predict a conformation sufficiently close to that of the co-crystallized ligand as the first conformation in over half of the cases.

Tables \ref{rfscore4:tbl-set-1-pdbbind-2007} and \ref{rfscore4:tbl-set-2-pdbbind-2012} enumerate the predictive performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2007 benchmark and the PDBbind v2013 blind benchmark, respectively. Figures \ref{rfscore4:set-1-pdbbind-2007} and \ref{rfscore4:set-2-pdbbind-2012} plot the same results graphically, where trn-1 means the model was trained in scheme 1, i.e. on crystal poses, and trn-2 means the model was trained in scheme 2, i.e. on docked poses. Likewise, tst-1 and tst-2 mean the model was tested on crystal and docked poses, respectively. Note that model 1 was trained on crystal poses and used out of the box without re-training, so its results of trn-1 are simply repeated for trn-2.

From these results on both benchmarks, several interesting phenomena are observed. First, for model 1, its performance tested on docked poses was always better than its performance tested on crystal poses, except for the Rs performance on the PDBbind v2007 benchmark. Vina performing better on docked poses is likely to be due to the fact that docked poses are by construction optima of the objective function spanned by the Vina score, which may favor prediction of docked poses over unoptimized crystal poses.

Second, for models 2, 3 and 4 trained on crystal poses, their performance tested on docked poses was always worse than their performance tested on crystal poses. This is well anticipated because of the impact of pose generation error.

Third, for models 2, 3 and 4 tested on docked poses, their performance was better when they were trained on docked poses than their counterparts trained on crystal poses. This implies that a simple and quick solution to improving performance on docked poses is to re-train the model on docked poses instead of on crystal poses.

Fourth, for models 2, 3 and 4 tested on crystal poses, the models trained on docked poses did not outperform their counterparts trained on crystal poses. This is also well anticipated due to the impact of pose generation error, and suggests that it is not feasible to improve the predictive performance on crystal poses by using docked poses for training.

Fifth, regardless of the training or test schemes, model 4 consistently outperformed model 3, which in turn outperformed model 2, which in turn outperformed model 1. It is remarkable that the best scoring function, RF::VinaElem, when trained on docked poses, achieved the highest performance in the literature on the PDBbind v2007 benchmark in the more common application of re-scoring docked poses. Here we denote this version of RF::VinaElem as RF-Score-v4 specifically for the purpose of binding affinity prediction given a docked pose. Importantly, since Vina and RF::Vina used the same features and were trained on the same data, RF::Vina performed much better in predicting binding affinity than the widely-used Vina while having the same applicability domain.

\subsection*{Dependency of RMSD with binding affinity prediction}

Next, we assessed the ability of each of the four models to predict the near-native pose from the up to nine docked poses output by Vina (Table \ref{rfscore4:near-native}). In other words, we would like to see if a model could correctly assign the best score to the particular docked pose having the lowest RMSD to the crystal pose out of at most nine docked poses. Interestingly, results show that Vina, although being the least accurate predictor of binding strength, turned out to be the best at predicting which docked pose is geometrically the closest to the crystal pose. This is probably due to the fact that, as explained previously, the best-scoring docked pose was resulted from the optimization of Vina's scoring function during redocking. In contrast, the presented machine-learning scoring functions, while excelling at binding affinity prediction, performed much worse than Vina at native pose prediction. This indicates that these two tasks, binding affinity prediction and native pose prediction, cannot be optimally covered by a single scoring function.

\subsection*{Using the re-docked poses of training complexes corrects for pose generation error}

Until now training on crystal and testing on re-docked poses without changing composition of training or test sets. Here still testing on re-docked poses (the best pose of each molecule defined as that with the lowest free energy of binding according to Vina), but now training on re-docked poses which improves test set performance with respect to the SF trained on crystal. I think this is set 2 and you have all the figures, put the best one here. Important result: first time as far as I know.

\subsection*{Results from other sets}

Until now using one docking pose per molecule, multi-pose schemes 5 and 6 did not improve, but it is worth communicating that they didn’t work. We also propose another type of schemes, where the features of a molecule are calculated from the entire ensemble of poses (if Vina returns less than nine poses, then the feature vector for that molecule is completed by repeating the pose with lowest Vina score as many times as poses are missing). The same for the test set. For models 2-3 there will be 91 features per molecule then (10VinaTerms*9poses + 1 Nrot) [NB: check the READMEs in your code]

\section*{Discussion}

Here we study the impact of pose generation error on classical and machine-learning scoring functions. Furthermore, we investigate which of these scoring functions is the most suitable for predicting the near-native pose, i.e. the docked pose most similar to the crystal pose. This kind of capability is referred to as ``docking power" in some other studies \cite{1411}.

The same models, materials and metrics were reused, with some slight adjustments specifically for the purpose of this study. Likewise, the numerical experiments were performed with AutoDock Vina \cite{595} as the classical scoring function because it is one of the most popular docking software, and RF-Score \cite{564} as the machine-learning scoring function because it has been vigorously studied in multiple aspects \cite{1281,1362,1370}. Note that although this chapter and the previous chapter share some similarities in terms of methods and materials, their applications are fundamentally different.

Our results show that pose generation error affects the accuracy of scoring functions, which is well anticipated. To minimize this negative impact, re-training the scoring functions on docked poses instead of crystal poses can be a straightforward solution. On the other hand, we find that although machine-learning scoring functions are generally good at binding affinity prediction, they do not perform as well as classical scoring functions on native pose prediction. This indicates that predictions of binding affinity and native pose are two different tasks and no single scoring function performs optimally for both tasks.

Although we only used RF in this study, we believe our conclusions are also applicable to SVR (support vector regression)-based scoring functions \cite{1295,963} as well as other machine-learning scoring functions.

RF-Score-v4 is freely available at http://istar.cse.cuhk.edu.hk/rf-score-4.tgz and http://crcm.marseille.inserm.fr/fileadmin/rf-score-4.tgz.

\section*{Conclusions}

This study has demonstrated that errors in pose generation generally introduce a degradation in the accuracy of scoring. One straightforward approach to enhancing predictive accuracy on docked poses is to re-train the scoring function also on docked poses. Furthermore, RF-Score-v4, essentially RF::VinaElem trained on docked poses, obtained the highest predictive performance on two PDBbind benchmarks in the common scenario where one has to predict the binding affinity of docked poses instead of those for crystal poses, usually because a crystal structure of the ligand is unavailable. Nevertheless, we observed that the presented machine-learning scoring functions did not perform as well as Vina in predicting the near native pose of a ligand. This could be due to the confounding factor that the docked poses were all generated and optimized by Vina. It is out of the scope of this study to investigate the generalization of these conclusions to other machine learning methods such as support vector regression, but we expect them to yield similar conclusions.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
The authors declare that they have no competing interests.

\section*{Author's contributions}
P.J.B. designed the study. H.L. wrote the manuscript with P.J.B. H.L. implemented the software and ran all the numerical experiments. All authors discussed results and commented on the manuscript.

This work has been carried out thanks to the support of the A*MIDEX grant (n$^{\circ}$ ANR-11-IDEX-0001-02) funded by the French Government $\ll$Investissements d’Avenir`$\gg$ program, the Direct Grant from the Chinese University of Hong Kong and the GRF Grant (Project Reference 414413) from the Research Grants Council of Hong Kong SAR.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file
\bibliography{../refworks}      % Bibliography file (usually '*.bib' )

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}

\begin{figure}[h]
%\includegraphics[natwidth=1400,natheight=1200,width=1.31\linewidth]{../rescoring2/4WAF-3K6.png}
\caption{\csentence{Example of pose generation error.} Top: crystal structure of PI3K$\alpha$ in complex of a tetrahydropyrazolo[1,5-a]pyrazine codenamed 3K6 (PDB ID: 4WAF). Bottom: re-docked pose of 3K6, generated by idock \cite{1362}. Hydrogen bonds are rendered as dashed cyan lines, and $\pi$ stackings are rendered as dashed pink lines. The RMSD (Root-Mean Square Deviation) between the co-crystallised pose and the re-docked pose of 3K6 is 1.15 \AA, which is a quantitative measure of pose generation error. These two plots were created by iview \cite{1366}, an interactive WebGL visualizer that circumvents the requirement of Java, yet supports the construction of macromolecular surface and the display of virtual reality effects and molecular interactions. iview is freely available at http://istar.cse.cuhk.edu.hk/iview/.
}
\label{fig:4WAF}
\end{figure}

\begin{figure}
\centering
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-1-pdbbind-2007-boxplot.pdf}
\caption{\csentence{Performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2007 benchmark.}}
\label{rfscore4:set-1-pdbbind-2007}
\end{figure}

\begin{figure}[h]
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-2-pdbbind-2012-boxplot.pdf}
\caption{\csentence{Performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2013 blind benchmark.}}
\label{rfscore4:set-2-pdbbind-2012}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
\section*{Tables}

\begin{table}[ht]
\caption{Performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2007 benchmark.}
\label{rfscore4:tbl-set-1-pdbbind-2007}
\begin{tabular}{cccrrrr}
\hline
Model & Training & Test & RMSE & SD & Rp & Rs\\
\hline
1 & Crystal & Crystal & 2.41 & 1.99 & 0.554 & 0.608\\
2 & Crystal & Crystal & 1.88 & 1.85 & 0.630 & 0.680\\
3 & Crystal & Crystal & 1.66 & 1.59 & 0.744 & 0.752\\
4 & Crystal & Crystal & 1.52 & 1.42 & 0.803 & 0.799\\
\hline
1 & Crystal & Docked  & 2.02 & 1.98 & 0.557 & 0.597\\
2 & Crystal & Docked  & 1.90 & 1.87 & 0.622 & 0.670\\
3 & Crystal & Docked  & 1.76 & 1.72 & 0.693 & 0.710\\
4 & Crystal & Docked  & 1.60 & 1.52 & 0.772 & 0.771\\
\hline
2 & Docked  & Crystal & 1.91 & 1.88 & 0.618 & 0.648\\
3 & Docked  & Crystal & 1.74 & 1.69 & 0.705 & 0.716\\
4 & Docked  & Crystal & 1.58 & 1.45 & 0.794 & 0.790\\
\hline
2 & Docked  & Docked  & 1.86 & 1.83 & 0.640 & 0.667\\
3 & Docked  & Docked  & 1.69 & 1.63 & 0.730 & 0.730\\
4 & Docked  & Docked  & 1.55 & 1.45 & 0.795 & 0.789\\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2013 blind benchmark.}
\label{rfscore4:tbl-set-2-pdbbind-2012}
\begin{tabular}{cccrrrr}
\hline
Model & Training & Test & RMSE & SD & Rp & Rs\\
\hline
1 & Crystal & Crystal & 2.30 & 1.81 & 0.406 & 0.414\\
2 & Crystal & Crystal & 1.67 & 1.67 & 0.535 & 0.521\\
3 & Crystal & Crystal & 1.54 & 1.54 & 0.629 & 0.593\\
4 & Crystal & Crystal & 1.43 & 1.43 & 0.689 & 0.662\\
\hline
1 & Crystal & Docked  & 1.87 & 1.78 & 0.437 & 0.432\\
2 & Crystal & Docked  & 1.70 & 1.69 & 0.520 & 0.505\\
3 & Crystal & Docked  & 1.61 & 1.60 & 0.585 & 0.549\\
4 & Crystal & Docked  & 1.49 & 1.49 & 0.656 & 0.633\\
\hline
2 & Docked  & Crystal & 1.69 & 1.69 & 0.521 & 0.509\\
3 & Docked  & Crystal & 1.62 & 1.61 & 0.580 & 0.560\\
4 & Docked  & Crystal & 1.48 & 1.47 & 0.669 & 0.650\\
\hline
2 & Docked  & Docked  & 1.68 & 1.68 & 0.524 & 0.509\\
3 & Docked  & Docked  & 1.59 & 1.59 & 0.594 & 0.553\\
4 & Docked  & Docked  & 1.47 & 1.48 & 0.665 & 0.643\\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Performance of the four models in near-native pose prediction.}
\label{rfscore4:near-native}
\begin{tabular}{ccccc}
\hline
& \multicolumn{2}{c}{PDBbind v2007 benchmark} & \multicolumn{2}{c}{PDBbind v2013 blind benchmark}\\
Model & \# & \% & \# & \%\\
\hline
1 & 94 & 48 & 208 & 54\\
2 & 59 & 30 & 142 & 37\\
3 & 53 & 27 & 119 & 31\\
4 & 59 & 30 & 141 & 37\\
\hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}

%\subsection*{cv.csv}
%This CSV file contains the PDB IDs and measured binding affinities of the protein-ligand complexes in the five partitions of PDBbind v2013 refined set for cross validation purpose.

\end{backmatter}
\end{document}
