%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins
%  [twocolumn]

\documentclass[twocolumn]{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
\usepackage{graphicx}
%\usepackage{rotating}
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%address
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\def\includegraphic{}
%\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Correcting the impact of docking pose generation error on binding affinity prediction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
%   corref={aff1},                       % id of corresponding address, if any
%   noteref={n1},                       % id's of article notes, if any
   email={jackyleehongjian@gmail.com}   % email address
]{\inits{HL}\fnm{Hongjian} \snm{Li}}
\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={ksleung@cse.cuhk.edu.hk}
]{\inits{KSL}\fnm{Kwong-Sak} \snm{Leung}}
\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={mhwong@cse.cuhk.edu.hk}
]{\inits{MHW}\fnm{Man-Hon} \snm{Wong}}
\author[
   addressref={aff2,aff3,aff4,aff5},
   corref={aff2,aff3,aff4,aff5},                      % id of corresponding address, if any
   email={email: pedro.ballester@inserm.fr; tel.: +33-486-977-265; fax: +33-486-977-499}
]{\inits{PJB}\fnm{Pedro J.} \snm{Ballester}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{
  \orgname{Department of Computer Science and Engineering, Chinese University of Hong Kong},
  \city{Hong Kong},
  \cny{China}
}
\address[id=aff2]{
  \orgname{Cancer Research Center of Marseille, INSERM U1068},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}
\address[id=aff3]{
  \orgname{Institut Paoli-Calmettes},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}
\address[id=aff4]{
  \orgname{Aix-Marseille Universit√©},
  \postcode{F-13284}
  \city{Marseille},
  \cny{France}
}
\address[id=aff5]{
  \orgname{CNRS UMR7258},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

%\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract}

\parttitle{Background}
Pose generation error is usually quantified as the difference between the geometry of the pose generated by the docking software and that of the same molecule co-crystallised with the considered protein. Surprisingly, the impact of this error on binding affinity prediction is yet to be systematically analysed across diverse protein-ligand complexes.

\parttitle{Results}
Against commonly-held views, pose generation error has generally a small impact on the accuracy in binding affinity prediction. This is also true for large pose generation errors and it is not only observed with machine-learning scoring functions, but also with classical scoring functions such as AutoDock Vina. Furthermore, we propose a procedure to correct this error which consists of calibrating the scoring functions with re-docked, rather than co-crystallised, poses. As a result, test set performance after this error-correcting procedure is virtually the same as that of predicting the binding affinity in the absence of pose generation error (i.e. on crystal structures). We evaluated several strategies, obtaining better results for those using a single docking pose per ligand.

\parttitle{Conclusions}
In practice, binding affinity prediction is carried out on the docking pose of a known binder rather than its co-crystallised pose. Our results suggest than pose generation error is in general far less damaging for binding affinity prediction than it is believed. From a practical standpoint, our proposed procedure largely corrects this error. The resulting machine-learning scoring function is freely available at http://istar.cse.cuhk.edu.hk/rf-score-4.tgz and http://crcm.marseille.inserm.fr/fileadmin/rf-score-4.tgz.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{molecular docking}
\kwd{binding affinity}
\kwd{drug discovery}
\kwd{machine learning}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Introduction}

Molecular docking tools are routinely utilised to predict the binding pose as well as the binding affinity of a ligand, usually a small organic molecule, bound to a target protein of interest. The predicted pose suggests putative intermolecular interactions that can be helpful to understand the mechanism of protein-ligand binding. On the other hand, predicted affinity prioritizes strong-binding ligands over weak-binding ones from a large library of compounds to evaluate.

A typical docking program implements a sampling algorithm to generate possible binding poses and a scoring function to estimate their binding affinity. The former operation is known as pose generation, and the latter is known as scoring. For example, modern docking tools such as AutoDock Vina \cite{595} and idock \cite{1362} are currently capable of generating near-native poses with a redocking success rate of over 50\% on three diverse benchmarks \cite{1362}.

Meanwhile, recent years have seen the emergence and prosperity of a new class of scoring functions that use machine learning techniques to increase the accuracy of binding affinity prediction, with RF-Score \cite{564} being the first machine-learning scoring function introducing a substantial improvement over classical scoring functions. Since then, several enhancements have been introduced, thereby resulting in RF-Score-v2 \cite{1370} and RF-Score-v3 \cite{1647}, and other relevant studies \cite{1432}. RF-Score has been utilised \cite{1281} to successfully discover a large number of innovative binders of antibacterial DHQase2 targets, demonstrating its practical utility. To promote its use, RF-Score-v3 has been incorporated into a user-friendly webserver called istar \cite{1362}, available at http://istar.cse.cuhk.edu.hk/idock, for large-scale docking-based prospective virtual screening. A recent study \cite{1663} has investigated the benefit of training machine-learning scoring functions with low-quality structural and interaction data.

In prospective virtual screening \cite{1362}, scoring of the docked poses of a molecule is required because the experimentally determined pose is not available in most cases. Therefore, accurate prediction of binding affinity of docked poses, rather than co-crystallised poses, is required for ranking compounds from screening libraries.

Pose generation error is typically measured by comparing the geometry of the pose generated by the docking software and that of the same molecule co-crystallised with the considered protein (Figure \ref{fig:4WAF}). The impact of this error on binding affinity prediction is yet to be systematically analysed across diverse protein-ligand complexes. In this study we investigate the impact of pose generation error on the predictive performance of both classical and machine-learning scoring functions, and propose a novel approach to correct such error. Furthermore, we release free software implementing such approach.

\section*{Methods}

This section introduces and motivates the use of four scoring functions building upon AutoDock Vina, two benchmarks to evaluate and compare performance of these scoring functions, the performance metrics, and the experimental setup.

\subsection*{Model 1 - AutoDock Vina}

AutoDock Vina \cite{595} was chosen as a baseline scoring function because of its popularity among the research community. Vina's popularity roots in its substantial improvements on both the average accuracy of the binding mode prediction and the running speed. Its remarkable performance in pose generation as well as its open source nature are other appealing aspects of this widely-used tool.

Like all classical scoring functions \cite{1647}, Vina assumes a predetermined functional form. Vina's score for the $k$th pose of a molecule is given by the predicted free energy of binding to the target protein and computed as:

\begin{equation}
\label{rescoring2:e_k}
e'_k=\frac{e_{k,inter}+e_{k,intra}-e_{1,intra}}{1+w_6N_{rot}}
\end{equation}

where

\begin{eqnarray}
\label{rescoring2:e_k_inter}
e_{k,inter} &=& w_1 \cdot Gauss1_k \nonumber \\
            &+& w_2 \cdot Gauss2_k \nonumber \\
		    &+& w_3 \cdot Repulsion_k \nonumber \\
		    &+& w_4 \cdot Hydrophobic_k \nonumber \\
		    &+& w_5 \cdot HBonding_k
\end{eqnarray}

\begin{eqnarray}
\label{rescoring2:w}
w_1 &=& -0.035579 \nonumber \\
w_2 &=& -0.005156 \nonumber \\
w_3 &=&  0.840245 \nonumber \\
w_4 &=& -0.035069 \nonumber \\
w_5 &=& -0.587439 \nonumber \\
w_6 &=&  0.05846
\end{eqnarray}

$e'_k$ is the predicted free energy of binding reported by the Vina software when scoring the $k$th docked pose. $e_{k,inter}$ and $e_{k,intra}$ are the inter-molecular and intra-molecular contributions, respectively, which have both the same functional form described in equation \ref{rescoring2:e_k_inter} but are summed over different atom pairs. The values for the six weights were calculated by OLS (Ordinary Least Squares) using a nonlinear optimisation algorithm as it has been the case in related force-field scoring functions \cite{1454}, although this process was not fully disclosed in the original publication \cite{595}. $N_{rot}$ is the calculated number of rotatable bonds. The predicted free energy of binding in kcal/mol units was converted into pKd units with $pK_d=-0.73349480509e$ so as to compare to binding affinities in $pK_d$ or $pK_i$ units. Mathematical expressions and further explanations can be found in \cite{1362}.

Unlike our previous study \cite{1647} on scoring crystal poses, where $k=1$ because only the crystal pose was considered, this study aims at training and testing on docked poses, so $k$ will range from 1 to 9 depending on the outcome of Vina's pose generation algorithm for each molecule. Thus $e_{k,intra}$ and $e_{1,intra}$ do not necessarily cancel out. As a result, the five terms from $e_{k,intra}$ were considered as additional features in models 2, 3 and 4.

\subsection*{Model 2 - MLR::Vina}

This model retains the 11 unweighted Vina terms (5 from $e_{k,inter}$, 5 from $e_{k,intra}$, and $N_{rot}$) as features, but changes the regression method to multiple linear regression (MLR), a regression model commonly adopted by classical scoring functions, such as empirical scoring functions. The use of MLR implies an additive functional form and therefore MLR::Vina is a classical scoring function \cite{1647}.

Vina's scoring function is not exactly a sum of energetic terms because $w_6\neq0$ (although the denominator of equation \ref{rescoring2:e_k} is close to 1 because of the low value of $w_6$. In order to make the problem amenable to MLR, we performed a grid search on $w_6$ and thereafter ran MLR on the remaining weights. More precisely, we sampled 101 values for $w_6$ from 0 to 1 with a step size of 0.01. Interestingly we found that the $w_6$ values of the best models were always between 0.000 and 0.030. Then we again sampled 31 values for $w_6$ in this range with a step size of 0.001, and used the $w_6$ value that resulted in the lowest RMSE (Root Mean Square Error) on the test set.

\subsection*{Model 3 - RF::Vina}

This model also retains the 11 unweighted Vina terms as features, but changes the regression method to Random Forest (RF) \cite{1309}, so as to implicitly learn the functional form from the data. Hence this model circumvents the modelling assumption of a predetermined functional form and thus allows to investigate the impact of such modelling assumption by comparing RF::Vina to MLR::Vina. Besides RF, other machine learning techniques such as SVR (Support Vector Regression) \cite{1295} can certainly be applied to this problem, although this is out of the scope of this study.

A RF is an ensemble of different decision trees randomly generated from the same training data via bootstraping \cite{1309}. RF trains its constituent trees using the CART algorithm \cite{1310}, and selects the best data split at each node of the tree from a typically small number (mtry) of randomly chosen features. In regression applications, the RF prediction is given by arithmetic mean of all the individual tree predictions in the forest.

For each value of the mtry control parameter from 1 to all 11 features, we built a RF model with 500 trees, which is a default value. The selected model was the one that led to the lowest RMSE on a subset of training data of each tree collectively known as the OOB (Out of Bag) data. Because RF is stochastic, this process was repeated ten times with ten different random seeds. The predictive performance was reported for the RF with the best seed that resulted in the lowest RMSE on the test set. Further details on RF model building in this context can be found in \cite{1647}.

\subsection*{Model 4 - RF::VinaElem}

This model retains RF as the regression method, but expands the feature set to 47 features by adding the 36 RF-Score \cite{564} features. Like in the training process of RF::Vina, the same ten seeds were used, and for a given random seed, a RF model for each mtry value from 1 to 47 was built and that with the lowest RMSE on OOB data was selected. The predictive performance was reported for the RF with the best seed that led to the lowest RMSE on the test set.

RF-Score features are defined as the occurrence count of intermolecular contacts between elemental atom types $i$ and $j$, as shown in equations \ref{rescoring2:x_ij} and \ref{rescoring2:x}, where $d_{kl}$ is the Euclidean distance between the $k$th protein atom of type $j$ and the $l$th ligand atom of type $i$ calculated from a structure; $K_j$ is the total number of protein atoms of type $j$ ($\#\{j\}=4$, considered protein atom types are C, N, O, S) and $L_i$ is the total number of ligand atoms of type $i$ ($\#\{i\}=9$, considered ligand atom types are C, N, O, F, P, S, Cl, Br, I); $\mathcal{H}$ is the Heaviside step function that counts contacts within a neighbourhood of $d_{cutoff}$ \AA. For instance, $x_{7,8}$ is the number of occurrences of protein oxygen atoms ($i$=7) hypothetically interacting with ligand nitrogen atoms ($j$=8) within a chosen neighbourhood. Full details on RF-Score features are available in \cite{564,1295}.

\begin{equation}
\label{rescoring2:x_ij}
x_{ij}=\sum_{k=1}^{K_j}\sum_{l=1}^{L_i}\mathcal{H}(d_{cutoff}-d_{kl})
\end{equation}

\begin{equation}
\label{rescoring2:x}
\mathbf x=\{x_{ij}\}\in N^{36}
\end{equation}

\subsection*{PDBbind v2007 benchmark}

We adopted the PDBbind v2007 benchmark \cite{1313}, arguably the most widely used \cite{1647,1432} for binding affinity prediction of diverse complexes. Its test set comprises 195 diverse complexes from the core set, whereas its training set comprises 1105 non-overlapping complexes from the refined set. Both the test and training sets come with measured binding affinities spanning more than 12 orders of magnitude. This benchmark has the advantage of permitting a direct comparison against the same four models that were trained and tested on crystal poses \cite{1647} of this benchmark.

\subsection*{PDBbind v2013 blind benchmark}

We also adopted the PDBbind v2013 blind benchmark \cite{1647}, a recently proposed new benchmark mimicking a blind test to provide a more realistic validation than the PDBbind v2007 benchmark. Its test set is composed of all the complexes in the PDBbind v2013 refined set that were not in the v2012 refined set, i.e. those 382 complexes that were newly added in the v2013 release. Its training set is simply the v2012 refined set, which contains 2897 complexes. By construction, this benchmark can be regarded as a blind test in that only data available until a certain year is used to build the scoring function that will be used to predict the binding affinity of future complexes as if these had not yet been measured. Consequently, the test set and training set do not overlap. Again, this benchmark has the advantage of permitting a direct comparison against the same four models that were trained and tested on crystal poses \cite{1647} of this benchmark.

In addition to the above training set, three more training sets were added in order to study how the performance of the four models would vary given different number of training complexes. The refined sets of PDBbind v2002 (N=792), v2007 (N=1300), v2010 (N=2057) and v2012 (N=2897) were chosen so that there is approximately the same number of complexes between consecutive releases. Complexes containing metal ions not supported by Vina were discarded. More details about this benchmark can be found in \cite{1647}.

\subsection*{Performance measures}

As usual \cite{1313}, predictive performance was quantified by the Root Mean Square Error (RMSE), Standard Deviation (SD), Pearson correlation (Rp) and Spearman rank-correlation (Rs) between predicted and measured binding affinities. Their mathematical expressions are shown in equations \ref{rescoring2:rmse}, \ref{rescoring2:sdev}, \ref{rescoring2:pcor}, and \ref{rescoring2:scor}. Given a scoring function $f$ and the features $\overrightarrow{x}^{(n)}$ characterising the $n$th complex out of $N$ complexes in the test set, $p^{(n)}=f(\overrightarrow{x}^{(n)})$ is the predicted binding affinity, $\{\hat{p}^{(n)}\}$ are the fitted values from the linear model between $\{y^{(n)}\}$ and $\{p^{(n)}\}$ on the test set, whereas $\{y_r^{(n)}\}$ and $\{p_r^{(n)}\}$ are the rankings of $\{y^{(n)}\}$ and $\{p^{(n)}\}$, respectively. Note that SD was calculated in a linear correlation, but RMSE was not. Lower values in RMSE and SD and higher values in Rp and Rs indicate a better predictive performance.

\begin{equation}
RMSE = \sqrt{\frac{1}{N}\sum_{n=1}^N(p^{(n)}-y^{(n)})^2}
\label{rescoring2:rmse}
\end{equation}

\begin{equation}
SD = \sqrt{\frac{1}{N-2}\sum_{n=1}^N(\hat{p}^{(n)}-y^{(n)})^2}
\label{rescoring2:sdev}
\end{equation}

\begin{equation}
\resizebox{.85\hsize}{!}{$R_p = \frac{N\sum_{n=1}^Np^{(n)}y^{(n)}-\sum_{n=1}^Np^{(n)}\sum_{n=1}^Ny^{(n)}}{\sqrt{(N\sum_{n=1}^N(p^{(n)})^2-(\sum_{n=1}^Np^{(n)})^2)(N\sum_{n=1}^N(y^{(n)})^2-(\sum_{n=1}^Ny^{(n)})^2)}}$}
\label{rescoring2:pcor}
\end{equation}

\begin{equation}
\resizebox{.85\hsize}{!}{$R_s = \frac{N\sum_{n=1}^Np_r^{(n)}y_r^{(n)}-\sum_{n=1}^Np_r^{(n)}\sum_{n=1}^Ny_r^{(n)}}{\sqrt{(N\sum_{n=1}^N(p_r^{(n)})^2-(\sum_{n=1}^Np_r^{(n)})^2)(N\sum_{n=1}^N(y_r^{(n)})^2-(\sum_{n=1}^Ny_r^{(n)})^2)}}$}
\label{rescoring2:scor}
\end{equation}

The Root Mean Square Deviation (RMSD) measures how geometrically different the redocked pose is from the corresponding co-crystallized pose of the same ligand molecule, i.e. the pose generation error. Suppose $N_a$ is the number of heavy atoms, $(x_c^{(n)}, y_c^{(n)}, z_c^{(n)})$ and $(x_d^{(n)}, y_d^{(n)}, z_d^{(n)})$ are the 3D coordinate of the $n$th heavy atom of the crystal and docked poses, respectively, the pose generation error is calculated as:

\begin{equation}
\resizebox{.85\hsize}{!}{$RMSD = \sqrt{\frac{1}{N_a}\sum_{n=1}^{N_a}[(x_c^{(n)}-x_d^{(n)})^2+(y_c^{(n)}-y_d^{(n)})^2+(z_c^{(n)}-z_d^{(n)})^2]}$}
\label{rescoring2:rmsd}
\end{equation}

\subsection*{Experimental setup}

To generate docked poses, each ligand in the two benchmarks was docked into the binding site of its target protein using Vina with its default settings. This process is known as redocking. The search space was defined by finding the smallest cubic box that covers the entire ligand and then by extending the box by 10\AA\ in X, Y, Z dimensions. Water molecules were removed, while metal ions recognized by Vina were retained as part of the protein. 

Redocking a ligand resulted in up to nine docked poses. Thus, the question arises of which pose best represents its molecule for calculating the values of the features. Here we evaluate different schemes referring to the specific pose from which the features are extracted. In scheme 1, the chosen pose is the crystal pose. In scheme 2, the chosen pose is the docked pose with the best Vina score, i.e. the one with the lowest Vina score in terms of estimated free energy of binding. We trained the four models on both crystal and docked poses (in both schemes), and tested them also on both crystal and docked poses (in both schemes).

To make our experiments comprehensive, we also evaluated additional schemes. In scheme 3, the chosen pose is the docked pose with the lowest RMSD. In scheme 4, the chosen pose is the docked pose with a Vina score closest the measured binding affinity. In scheme 5, the chosen poses are all the 9 docked poses, which hence results in a 9 times larger feature set (the number of features is 91 for models 2 and 3, and 415 for model 4). For ligands with less than 9 docked poses outputted, the features extracted from the pose with the lowest Vina score are repeated as many times as poses are missing. In scheme 6, the chosen poses are the 2 docked poses with the lowest and the second lowest Vina score, which hence results in a double-sized feature set (the number of features is 21 for models 2 and 3, and 93 for model 4). For ligands with less than 2 docked poses outputted, the features extracted from the pose with the lowest Vina score are repeated. The rationale of introducing these schemes is that, schemes 1 to 4 help to determine which particular pose would be useful for improving predictive accuracy on docked poses, while schemes 5 and 6 help to examine the effect of pose ensemble instead of a single pose.

Hereafter whenever we mention the docked pose, we implicitly refer to the one with the best Vina score (scheme 2), if not specified otherwise.

\section*{Results}

\subsection*{Pose generation error slightly worsens binding affinity prediction}

This question was analysed by using schemes 1 and 2. After redocking by Vina, we used RMSD to quantify the pose generation error, i.e. how different the 3D geometry of the redocked pose is from the corresponding crystal pose of the same ligand molecule. A RMSD value of 2\AA\ was used as a commonly accepted threshold for a correctly reproduced crystal pose. 101 out of the 195 ligands (52\%) in the PDBbind v2007 benchmark and 219 out of the 382 ligands (57\%) in the PDBbind v2013 blind benchmark had their best-scoring docked pose with RMSD \textless 2\AA. When all the docked poses of the molecule were considered, the redocking success rate of the two benchmarks increased to 76\% (149 out of 195) and 81\% (311 out of 382), respectively. These results are consistent with the previous results obtained in \cite{1362}, where Vina managed to predict a pose sufficiently close to that of the co-crystallized ligand as the best-scoring pose in over half of the cases.

Tables \ref{rescoring2:set-1-pdbbind-2007-raw} and \ref{rescoring2:set-2-pdbbind-2012-raw} show the predictive performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2007 benchmark and the PDBbind v2013 blind benchmark, respectively. Figures \ref{rescoring2:set-1-pdbbind-2007-boxplot} and \ref{rescoring2:set-2-pdbbind-2012-boxplot} visualize the same results using boxplots, as RF models are stochastic. Note that Vina (model 1) was trained on crystal poses and used out-of-the-box without re-training, so its results for training scheme 2 are simply a duplicate of its results for training scheme 1.

From these results, several interesting observations can be made. First, for model 1, its performance tested on docked poses was always better than its performance tested on crystal poses (except for just a small degradation in the Rs performance on the PDBbind v2007 benchmark). Particularly, the RMSE error was greatly dropped from 2.41 to 2.02 on the PDBbind v2007 benchmark and from 2.30 to 1.87 on the PDBbind v2013 blind benchmark. The result that Vina made better prediction of binding affinity from docked poses than from crystal poses is possibly due to the fact that docked poses are by construction optima of the objective function spanned by the Vina score, which may favor prediction of docked poses over unoptimized crystal poses.

Second, for models 2, 3 and 4 trained on crystal poses, their performance tested on docked poses was always worse than their performance tested on crystal poses. This is well anticipated because of the presence of pose generation error. For instance, on the PDBbind v2013 blind benchmark, model 4 trained on crystal poses obtained RMSE=1.43, SD=1.43, Rp=0.689, Rs=0.662 when tested on crystal poses (Additional file 1). Its performance degraded when tested on docked poses of the same molecules with RMSE=1.49, SD=1.49, Rp=0.656, Rs=0.633 (Additional file 2).

Third, for models 2, 3 and 4 tested on docked poses, their performance was better when they were trained on docked poses than their counterparts trained on crystal poses. For instance, on the PDBbind v2013 blind benchmark, model 4 trained on docked poses obtained RMSE=1.47, SD=1.48, Rp=0.665, Rs=0.643 when tested on docked poses (Additional file 3). This means that a way to improve performance on docked poses is to train the model on docked poses instead of on crystal poses. In practice, different scoring functions can be built depending on whether one wants to score crystal poses or docked poses.

Fourth, for models 2, 3 and 4 tested on crystal poses, the models trained on docked poses did not outperform their counterparts trained on crystal poses (Additional file 4). This is also well anticipated due to the impact of pose generation error, and suggests that it is not feasible to improve the predictive performance on crystal poses by using docked poses for training. To sum up, if the application is to score a crystal pose, it would be better to train the scoring function on crystal poses; and if the application is to score a docked pose, it would be better to train the scoring function on docked poses.

Fifth, regardless of the training or test schemes, model 4 consistently outperformed model 3, which in turn outperformed model 2, which in turn outperformed model 1. It is remarkable that the best scoring function, RF::VinaElem, when trained on docked poses, achieved the highest performance in the literature on the PDBbind v2007 benchmark in the more common application of re-scoring docked poses, as it is required when carrying out docking-based prospective virtual screening. Here we denote this version of RF::VinaElem as RF-Score-v4 specifically for the purpose of binding affinity prediction given a docked pose from Vina. Importantly, since Vina and RF::Vina used the same features and were trained on the same data, RF::Vina performed much better in predicting binding affinity than the widely-used Vina software while having the same applicability domain.

\subsection*{Training with more complexes on docked poses still improves performance}

In a previous study \cite{1647} we showed that training RF models with larger datasets greatly improved their predictive performance on scoring crystal poses, while the performance of MLR::Vina nearly stayed flat. Here we observe similar results when the models were trained on docked poses and tested also on docked poses of the PDBbind v2013 blind benchmark. As shown in Figure \ref{rescoring2:set-2-trn-2-tst-2-boxplot}, when more complexes were used for model training, RF::VinaElem consistently increased its predictive accuracy in terms of all the four metrics. In contrast, for MLR::Vina, its accuracy improvement obtained from larger training sets was just marginal. Not only the performance gap between MLR::Vina and RF::VinaElem is substantial, but grows as more data is available for training, thus increasing the importance of using RF in scoring function development. More importantly, the availability of crystal poses is limited by the number of experimentally resolved structures, whereas docked poses can be easily generated by docking tools. This means that, by using docked poses for training, the training data size can be remarkably larger than limiting the training data to crystal poses, and therefore even higher performance could be achieved.

\subsection*{Correlation between pose generation error and binding affinity prediction error is low}

We analyse how different RMSD values affect binding affinity prediction by comparing the RMSD of the docked pose with the individual absolute error in its binding affinity prediction by the four models (note that the square root of the summation of the square of these errors is the RMSE measure). It is widely believed that the higher the pose generation error, the larger the error on predicting that pose will be. Nevertheless, this is not the case here.

Figures \ref{rescoring2:set-1-pdbbind-2007-trn-2-tst-2-de} and \ref{rescoring2:set-2-pdbbind-2012-trn-2-tst-2-de} plot this information for each of the four scoring functions trained and tested on docked poses of the two benchmarks, respectively. Strikingly, the four scoring functions are particularly robust to pose generation error, with reasonably accurate prediction still being obtained in poses with RMSDs of almost 15. The Rp and Rs values stated at the top of these plots quantify how little the pose generation error generally correlates with the binding affinity prediction error, at least when using these scoring functions. This is likely to be connected to uncertainty associated to relating a static crystal structure of the complex with its measured binding affinity which is the outcome of the dynamic process of binding, as discussed in \cite{1370}. To the best of our knowledge, these behaviour has not been communicated yet for classical scoring functions, which is very surprising given that these have been around for more than 30 years. On the other hand, it is noteworthy that, while the binding affinities of some complexes are very well predicted (pKd error close to 0), some others have errors of more than 7 pKd units (see the topleft plots for Vina).

\subsection*{Using multiple docked poses for training does not improve performance}

Besides using either crystal or docked poses (schemes 1 and 2) for training, we additionally evaluated several strategies (schemes 3, 4, 5 and 6), aiming to see if using other docked pose, or even multiple docked poses, could possibly increase the predictive performance of the resultant models. Recall that scheme 3 uses the docked pose with the lowest RMSD, scheme 4 uses the docked pose with a Vina score closest the measured binding affinity, scheme 5 uses all the 9 docked poses, and scheme 6 uses the two top-scoring docked poses. The results on the two benchmarks are shown in Tables \ref{rescoring2:set-1-pdbbind-2007-trn-3456-tst-1256-raw} and \ref{rescoring2:set-2-pdbbind-2012-trn-3456-tst-1256-raw}. Note that schemes 3 and 4 could not be used as test schemes because neither the RMSD nor the measured binding affinity of test set complexes are known in practice. Hence, models 2, 3 and 4 trained in schemes 3 and 4 were tested in schemes 1 and 2 instead. On the other hand, models trained in schemes 5 and 6 can only be tested in schemes 5 and 6, respectively, because their feature set has a different size than that of schemes 1 to 4.


[Talk about schemes 3, 4, 5, 6] Until now using one docking pose per molecule, multi-pose schemes 5 and 6 did not improve, but it is worth communicating that they did not work.

\section*{Discussion}

To the best of our knowledge, this is the first study that systematically investigates the impact of pose generation error on binding affinity prediction for both classical and machine-learning scoring functions.

Here we study the impact of pose generation error on classical and machine-learning scoring functions. Furthermore, we investigate the dependency of the RMSD of test set complexes with binding affinity prediction. % TODO: add the conclusion.

Our results show that pose generation error only introduces a small degradation in the accuracy of scoring functions. To minimize this negative impact, we found that re-training the scoring functions on docked poses instead of crystal poses results in correcting a substantial part of this degradation. Until now existing machine-learning scoring functions are trained on crystal poses and tested on re-docked poses without changing composition of training or test sets. Here still testing on re-docked poses, but now training on re-docked poses which improves test set performance with respect to the SF trained on crystal.

Scheme 2 is determinable, i.e. the lowest Vina score. Schemes 3 and 4 are not because RMSD and binding affinity are not known in advance. So scheme 2 is practically implementable while schemes 3 and 4 are not.

Another contribution of this study is the release of software implementing RF::VinaElem trained on docked poses so that it can be directly used by the large number of Vina users (poses from other docking programs can also be re-scored by our software once these are converted to AutoDock‚Äôs pdbqt format). With this purpose, we have trained the best of our models on the most comprehensive set of high-quality complexes (RF-Score-v4; RF::VinaElem trained on the 3441 complexes from 2014 refined set) and implemented it as easy-to-use software that directly re-scores Vina-generated poses. See the abstract for availability and the README file therein for operating instructions.

Although we only used RF in this study as a proof of concept, we believe our conclusions are also applicable to SVR (Support Vector Regression)-based scoring functions \cite{1295,963} as well as other machine-learning scoring functions, which could possibly achieve even higher correction rates.

\section*{Conclusions}

This study has demonstrated that errors in pose generation generally introduce a degradation in the accuracy of scoring. One straightforward approach to enhancing predictive accuracy on docked poses is to re-train the scoring function also on docked poses. Furthermore, RF-Score-v4, essentially RF::VinaElem trained on docked poses, obtained the highest predictive performance on two PDBbind benchmarks in the common scenario where one has to predict the binding affinity of docked poses instead of those for crystal poses, usually because a crystal structure of the ligand is unavailable. % TODO: put the conclusions of the dependency of RMSD and applying other schemes

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
The authors declare that they have no competing interests.

\section*{Author's contributions}
P.J.B. designed the study. H.L. wrote the manuscript with P.J.B. H.L. implemented the software and ran all the numerical experiments. All authors discussed results and commented on the manuscript.

This work has been carried out thanks to the support of the A*MIDEX grant (n$^{\circ}$ ANR-11-IDEX-0001-02) funded by the French Government $\ll$Investissements d‚ÄôAvenir`$\gg$ program, the Direct Grant from the Chinese University of Hong Kong and the GRF Grant (Project Reference 414413) from the Research Grants Council of Hong Kong SAR.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file
\bibliography{../refworks}      % Bibliography file (usually '*.bib' )

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}

\begin{figure}[h]
%\includegraphics[natwidth=1400,natheight=1200,width=1.31\linewidth]{../rescoring2/4WAF-3K6.png}
\caption{\csentence{Example of pose generation error.} Top: crystal structure of PI3K$\alpha$ in complex of a tetrahydropyrazolo[1,5-a]pyrazine codenamed 3K6 (PDB ID: 4WAF). Bottom: re-docked pose of 3K6, generated by idock \cite{1362}. Hydrogen bonds are rendered as dashed cyan lines, and $\pi$ stackings are rendered as dashed pink lines. The RMSD (Root-Mean Square Deviation) between the co-crystallised pose and the re-docked pose of 3K6 is 1.15 \AA, which is a quantitative measure of pose generation error. These two plots were created by iview \cite{1366}, an interactive WebGL visualizer that circumvents the requirement of Java, yet supports the construction of macromolecular surface and the display of virtual reality effects and molecular interactions. iview is freely available at http://istar.cse.cuhk.edu.hk/iview/.
}
\label{fig:4WAF}
\end{figure}

\begin{figure}
\centering
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-1-pdbbind-2007-boxplot.pdf}
\caption{\csentence{Box plots of performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses of the PDBbind v2007 benchmark.} Model 1 is AutoDock Vina, model 2 is MLR::Vina, model 3 is RF::Vina, and model 4 is RF::VinaElem. In the x axis, trn:1 means the four models were trained in scheme 1, i.e. on crystal poses, and trn:2 means the four models were trained in scheme 2, i.e. on docked poses. Likewise, tst-1 and tst-2 mean the four models were tested on crystal and docked poses, respectively. Model 1 was executed out-of-the-box, so its results for training scheme 1 were repeated for training scheme 2.}
\label{rescoring2:set-1-pdbbind-2007-boxplot}
\end{figure}

\begin{figure}[h]
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-2-pdbbind-2012-boxplot.pdf}
\caption{\csentence{Box plots of performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses of the PDBbind v2013 blind benchmark.} The same notations are applied here as in Figure \ref{rescoring2:set-1-pdbbind-2007-boxplot}.}
\label{rescoring2:set-2-pdbbind-2012-boxplot}
\end{figure}

\begin{figure}[h]
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-2-trn-2-tst-2-boxplot.pdf}
\caption{\csentence{Box plots of performance of the four models trained on docked poses and tested also on docked poses of the PDBbind v2013 blind benchmark, with four incrementally-sized training sets.} Model 1 is AutoDock Vina, model 2 is MLR::Vina, model 3 is RF::Vina, and model 4 is RF::VinaElem.}
\label{rescoring2:set-2-trn-2-tst-2-boxplot}
\end{figure}

\begin{figure}[h]
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-1-pdbbind-2007-trn-2-tst-2-de.pdf}
\caption{\csentence{Correlation plots of predicted binding affinity absolute errors achieved by the four models trained on docked poses and tested on docked poses of the PDBbind v2007 benchmark against the RMSD values from redocking the 195 test set complexes by Vina.} Model 1 is AutoDock Vina, model 2 is MLR::Vina, model 3 is RF::Vina, and model 4 is RF::VinaElem.}
\label{rescoring2:set-1-pdbbind-2007-trn-2-tst-2-de}
\end{figure}

\begin{figure}[h]
%\includegraphics[natwidth=720,natheight=720,width=0.98\linewidth]{../rescoring2/set-2-pdbbind-2012-trn-2-tst-2-de.pdf}
\caption{\csentence{Correlation plots of predicted binding affinity absolute errors achieved by the four models trained on docked poses and tested on docked poses of the PDBbind v2013 blind benchmark against the RMSD values from redocking the 382 test set complexes by Vina.} Model 1 is AutoDock Vina, model 2 is MLR::Vina, model 3 is RF::Vina, and model 4 is RF::VinaElem.}
\label{rescoring2:set-2-pdbbind-2012-trn-2-tst-2-de}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
\section*{Tables}

\begin{table}[ht]
\caption{Performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2007 benchmark.}
\label{rescoring2:set-1-pdbbind-2007-raw}
\begin{tabular}{cccrrrr}
\hline
Model & Training & Test & RMSE & SD & Rp & Rs\\
\hline
1 & Crystal & Crystal & 2.41 & 1.99 & 0.554 & 0.608\\
2 & Crystal & Crystal & 1.88 & 1.85 & 0.630 & 0.680\\
3 & Crystal & Crystal & 1.66 & 1.59 & 0.744 & 0.752\\
4 & Crystal & Crystal & 1.52 & 1.42 & 0.803 & 0.799\\
\hline
1 & Crystal & Docked  & 2.02 & 1.98 & 0.557 & 0.597\\
2 & Crystal & Docked  & 1.90 & 1.87 & 0.622 & 0.670\\
3 & Crystal & Docked  & 1.76 & 1.72 & 0.693 & 0.710\\
4 & Crystal & Docked  & 1.60 & 1.52 & 0.772 & 0.771\\
\hline
2 & Docked  & Crystal & 1.91 & 1.88 & 0.618 & 0.648\\
3 & Docked  & Crystal & 1.74 & 1.69 & 0.705 & 0.716\\
4 & Docked  & Crystal & 1.58 & 1.45 & 0.794 & 0.790\\
\hline
2 & Docked  & Docked  & 1.86 & 1.83 & 0.640 & 0.667\\
3 & Docked  & Docked  & 1.69 & 1.63 & 0.730 & 0.730\\
4 & Docked  & Docked  & 1.55 & 1.45 & 0.795 & 0.789\\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Performance of the four models trained on crystal and docked poses and tested also on crystal and docked poses on the PDBbind v2013 blind benchmark.}
\label{rescoring2:set-2-pdbbind-2012-raw}
\begin{tabular}{cccrrrr}
\hline
Model & Training & Test & RMSE & SD & Rp & Rs\\
\hline
1 & Crystal & Crystal & 2.30 & 1.81 & 0.406 & 0.414\\
2 & Crystal & Crystal & 1.67 & 1.67 & 0.535 & 0.521\\
3 & Crystal & Crystal & 1.54 & 1.54 & 0.629 & 0.593\\
4 & Crystal & Crystal & 1.43 & 1.43 & 0.689 & 0.662\\
\hline
1 & Crystal & Docked  & 1.87 & 1.78 & 0.437 & 0.432\\
2 & Crystal & Docked  & 1.70 & 1.69 & 0.520 & 0.505\\
3 & Crystal & Docked  & 1.61 & 1.60 & 0.585 & 0.549\\
4 & Crystal & Docked  & 1.49 & 1.49 & 0.656 & 0.633\\
\hline
2 & Docked  & Crystal & 1.69 & 1.69 & 0.521 & 0.509\\
3 & Docked  & Crystal & 1.62 & 1.61 & 0.580 & 0.560\\
4 & Docked  & Crystal & 1.48 & 1.47 & 0.669 & 0.650\\
\hline
2 & Docked  & Docked  & 1.68 & 1.68 & 0.524 & 0.509\\
3 & Docked  & Docked  & 1.59 & 1.59 & 0.594 & 0.553\\
4 & Docked  & Docked  & 1.47 & 1.48 & 0.665 & 0.643\\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Performance of the four models trained in schemes 3, 4, 5, 6 and tested in schemes 1, 2, 5, 6 on the PDBbind v2007 benchmark.}
\label{rescoring2:set-1-pdbbind-2007-trn-3456-tst-1256-raw}
\begin{tabular}{cccrrrr}
\hline
Model & Training & Test & RMSE & SD & Rp & Rs\\
\hline
2 & 3 & 1 & 1.89 & 1.85 & 0.629 & 0.675\\
3 & 3 & 1 & 1.76 & 1.73 & 0.691 & 0.694\\
4 & 3 & 1 & 1.58 & 1.45 & 0.795 & 0.792\\
\hline
2 & 3 & 2 & 1.88 & 1.85 & 0.630 & 0.661\\
3 & 3 & 2 & 1.72 & 1.68 & 0.711 & 0.714\\
4 & 3 & 2 & 1.57 & 1.45 & 0.793 & 0.780\\
\hline
2 & 4 & 1 & 1.93 & 1.93 & 0.589 & 0.648\\
3 & 4 & 1 & 1.81 & 1.80 & 0.656 & 0.669\\
4 & 4 & 1 & 1.63 & 1.53 & 0.769 & 0.769\\
\hline
2 & 4 & 2 & 1.94 & 1.93 & 0.589 & 0.636\\
3 & 4 & 2 & 1.79 & 1.75 & 0.682 & 0.686\\
4 & 4 & 2 & 1.63 & 1.53 & 0.769 & 0.762\\
\hline
2 & 5 & 5 & 1.90 & 1.89 & 0.609 & 0.641\\
3 & 5 & 5 & 1.74 & 1.70 & 0.700 & 0.699\\
4 & 5 & 5 & 1.65 & 1.55 & 0.760 & 0.754\\
\hline
2 & 6 & 6 & 1.86 & 1.83 & 0.640 & 0.670\\
3 & 6 & 6 & 1.73 & 1.69 & 0.707 & 0.707\\
4 & 6 & 6 & 1.60 & 1.49 & 0.780 & 0.769\\
\hline
\end{tabular}
\end{table}

\begin{table}[ht]
\caption{Performance of the four models trained in schemes 3, 4, 5, 6 and tested in schemes 1, 2, 5, 6 on the PDBbind v2013 blind benchmark.}
\label{rescoring2:set-2-pdbbind-2012-trn-3456-tst-1256-raw}
\begin{tabular}{cccrrrr}
\hline
Model & Training & Test & RMSE & SD & Rp & Rs\\
\hline
2 & 3 & 1 & 1.70 & 1.69 & 0.521 & 0.511\\
3 & 3 & 1 & 1.60 & 1.58 & 0.602 & 0.575\\
4 & 3 & 1 & 1.48 & 1.48 & 0.666 & 0.643\\
\hline
2 & 3 & 2 & 1.69 & 1.69 & 0.523 & 0.509\\
3 & 3 & 2 & 1.59 & 1.58 & 0.601 & 0.562\\
4 & 3 & 2 & 1.49 & 1.49 & 0.655 & 0.635\\
\hline
2 & 4 & 1 & 1.88 & 1.80 & 0.413 & 0.415\\
3 & 4 & 1 & 1.72 & 1.71 & 0.499 & 0.477\\
4 & 4 & 1 & 1.57 & 1.57 & 0.610 & 0.589\\
\hline
2 & 4 & 2 & 1.77 & 1.75 & 0.468 & 0.447\\
3 & 4 & 2 & 1.70 & 1.66 & 0.544 & 0.508\\
4 & 4 & 2 & 1.58 & 1.57 & 0.611 & 0.582\\
\hline
2 & 5 & 5 & 1.65 & 1.65 & 0.550 & 0.526\\
3 & 5 & 5 & 1.58 & 1.58 & 0.603 & 0.578\\
4 & 5 & 5 & 1.49 & 1.50 & 0.653 & 0.633\\
\hline
2 & 6 & 6 & 1.68 & 1.68 & 0.526 & 0.514\\
3 & 6 & 6 & 1.57 & 1.57 & 0.608 & 0.581\\
4 & 6 & 6 & 1.47 & 1.48 & 0.665 & 0.643\\
\hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}

\subsection*{Additional file 1 --- set-2-pdbbind-2012-trn-1-tst-1-yp.pdf}
Correlation plots of measured and predicted binding affinities by the four models trained on crystal poses and tested on crystal poses of the PDBbind v2013 blind benchmark.

\subsection*{Additional file 2 --- set-2-pdbbind-2012-trn-1-tst-2-yp.pdf}
Correlation plots of measured and predicted binding affinities by the four models trained on crystal poses and tested on docked poses of the PDBbind v2013 blind benchmark.

\subsection*{Additional file 3 --- set-2-pdbbind-2012-trn-2-tst-2-yp.pdf}
Correlation plots of measured and predicted binding affinities by the four models trained on docked poses and tested on docked poses of the PDBbind v2013 blind benchmark.

\subsection*{Additional file 4 --- set-2-pdbbind-2012-trn-2-tst-1-yp.pdf}
Correlation plots of measured and predicted binding affinities by the four models trained on docked poses and tested on crystal poses of the PDBbind v2013 blind benchmark.

%\subsection*{Additional file X --- set-2-trn-1-tst-2-boxplot.pdf}
%Box plots of performance of the four models trained on crystal poses and tested on docked poses of the PDBbind v2013 blind benchmark, with four incrementally-sized training sets.

%\subsection*{Additional file X --- set-1-pdbbind-2007-trn-1-tst-2-de.pdf}
%Correlation plots of predicted binding affinity absolute errors achieved by the four models trained on crystal poses and tested on docked poses of the PDBbind v2007 benchmark against the RMSD values from redocking the 195 test set complexes by Vina.

%\subsection*{Additional file X --- set-2-pdbbind-2012-trn-1-tst-2-de.pdf}
%Correlation plots of predicted binding affinity absolute errors achieved by the four models trained on crystal poses and tested on docked poses of the PDBbind v2013 blind benchmark against the RMSD values from redocking the 382 test set complexes by Vina.

\end{backmatter}
\end{document}
