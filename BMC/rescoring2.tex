%% BioMed_Central_Tex_Template_v1.06
%%                                      %
%  bmc_article.tex            ver: 1.06 %
%                                       %

%%IMPORTANT: do not delete the first line of this template
%%It must be present to enable the BMC Submission system to
%%recognise this template!!

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                     %%
%%  LaTeX template for BioMed Central  %%
%%     journal article submissions     %%
%%                                     %%
%%          <8 June 2012>              %%
%%                                     %%
%%                                     %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                                                 %%
%% For instructions on how to fill out this Tex template           %%
%% document please refer to Readme.html and the instructions for   %%
%% authors page on the biomed central website                      %%
%% http://www.biomedcentral.com/info/authors/                      %%
%%                                                                 %%
%% Please do not use \input{...} to include other tex files.       %%
%% Submit your LaTeX manuscript as one .tex document.              %%
%%                                                                 %%
%% All additional figures and files should be attached             %%
%% separately and not embedded in the \TeX\ document itself.       %%
%%                                                                 %%
%% BioMed Central currently use the MikTex distribution of         %%
%% TeX for Windows) of TeX and LaTeX.  This is available from      %%
%% http://www.miktex.org                                           %%
%%                                                                 %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% additional documentclass options:
%  [doublespacing]
%  [linenumbers]   - put the line numbers on margins
%  [twocolumn]

\documentclass[twocolumn]{bmcart}

%%% Load packages
%\usepackage{amsthm,amsmath}
%\RequirePackage{natbib}
%\RequirePackage{hyperref}
\usepackage[utf8]{inputenc} %unicode support
%\usepackage{graphicx}
%\usepackage{rotating}
%\usepackage[applemac]{inputenc} %applemac support if unicode package fails
%\usepackage[latin1]{inputenc} %UNIX support if unicode package fails


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                             %%
%%  If you wish to display your graphics for   %%
%%  your own use using includegraphic or       %%
%%  includegraphics, then comment out the      %%
%%  following two lines of code.               %%
%%  NB: These line *must* be included when     %%
%%  submitting to BMC.                         %%
%%  All figure files must be submitted as      %%
%%  separate graphics through the BMC          %%
%%  submission process, not included in the    %%
%%  submitted article.                         %%
%%                                             %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%


%\def\includegraphic{}
%\def\includegraphics{}



%%% Put your definitions there:
\startlocaldefs
\endlocaldefs


%%% Begin ...
\begin{document}

%%% Start of article front matter
\begin{frontmatter}

\begin{fmbox}
\dochead{Research}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the title of your article here     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\title{Correcting the docking pose generation error on binding affinity prediction}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors here                   %%
%%                                          %%
%% Specify information, if available,       %%
%% in the form:                             %%
%%   <key>={<id1>,<id2>}                    %%
%%   <key>=                                 %%
%% Comment or delete the keys which are     %%
%% not used. Repeat \author command as much %%
%% as required.                             %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
%   corref={aff1},                       % id of corresponding address, if any
%   noteref={n1},                       % id's of article notes, if any
   email={jackyleehongjian@gmail.com}   % email address
]{\inits{HL}\fnm{Hongjian} \snm{Li}}
\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={ksleung@cse.cuhk.edu.hk}
]{\inits{KSL}\fnm{Kwong-Sak} \snm{Leung}}
\author[
   addressref={aff1},                   % id's of addresses, e.g. {aff1,aff2}
   email={mhwong@cse.cuhk.edu.hk}
]{\inits{MHW}\fnm{Man-Hon} \snm{Wong}}
\author[
   addressref={aff2,aff3,aff4,aff5},
   corref={aff2,aff3,aff4,aff5},                      % id of corresponding address, if any
   email={E-Mail: pedro.ballester@inserm.fr; Tel.: +33-486-977-265; Fax: +33-486-977-499}
]{\inits{PJB}\fnm{Pedro J.} \snm{Ballester}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter the authors' addresses here        %%
%%                                          %%
%% Repeat \address commands as much as      %%
%% required.                                %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\address[id=aff1]{
  \orgname{Department of Computer Science and Engineering, Chinese University of Hong Kong},
  \city{Hong Kong},
  \cny{China}
}
\address[id=aff2]{
  \orgname{Cancer Research Center of Marseille, INSERM U1068},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}
\address[id=aff3]{
  \orgname{Institut Paoli-Calmettes},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}
\address[id=aff4]{
  \orgname{Aix-Marseille Université},
  \postcode{F-13284}
  \city{Marseille},
  \cny{France}
}
\address[id=aff5]{
  \orgname{CNRS UMR7258},
  \postcode{F-13009}
  \city{Marseille},
  \cny{France}
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Enter short notes here                   %%
%%                                          %%
%% Short notes will be after addresses      %%
%% on first page.                           %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{artnotes}
%\note{Sample of title note}     % note to the article
%\note[id=n1]{Equal contributor} % note, connected to author
\end{artnotes}

%\end{fmbox}% comment this for two column layout

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Abstract begins here                 %%
%%                                          %%
%% Please refer to the Instructions for     %%
%% authors on http://www.biomedcentral.com  %%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{abstractbox}

\begin{abstract}

\parttitle{Background}
Pose generation error is usually measured by comparing the geometry of the pose generated by the docking software and that of the same molecule co-crystallised with the considered protein. Surprisingly, the impact of this error on binding affinity prediction is yet to be systematically analysed across diverse protein-ligand complexes.

\parttitle{Results}
Against commonly-held views, pose generation error has generally a small impact on the accuracy in binding affinity prediction. This is also true for large pose generation errors and it is not only observed with machine-learning scoring functions, but also with classical scoring functions such as AutoDock Vina. Furthermore, we propose a procedure to correct for this error which consists of calibrating the scoring functions with re-docked, rather than co-crystallised, poses. As a result, test set performance after this error-correcting procedure is virtually the same as that of predicting the binding affinity in the absence of pose generation error (i.e. on crystal structures). We evaluated several strategies, obtaining better results for those using a single docking pose per ligand.

\parttitle{Conclusions}
In practice, binding affinity prediction is carried out on the docking pose of a known binder rather than its co-crystallised pose. Our results suggest than pose generation error is in general far less damaging for binding affinity prediction than it is believed. From a practical standpoint, the proposed procedure largely corrects this error. The resulting machine-learning scoring function is freely available at http://istar.cse.cuhk.edu.hk/rf-score-4.tgz and http://crcm.marseille.inserm.fr/fileadmin/rf-score-4.tgz.

\end{abstract}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The keywords begin here                  %%
%%                                          %%
%% Put each keyword in separate \kwd{}.     %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{keyword}
\kwd{molecular docking}
\kwd{binding affinity}
\kwd{drug discovery}
\kwd{machine learning}
\end{keyword}

% MSC classifications codes, if any
%\begin{keyword}[class=AMS]
%\kwd[Primary ]{}
%\kwd{}
%\kwd[; secondary ]{}
%\end{keyword}

\end{abstractbox}
%
\end{fmbox}% uncomment this for twcolumn layout

\end{frontmatter}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% The Main Body begins here                %%
%%                                          %%
%% Please refer to the instructions for     %%
%% authors on:                              %%
%% http://www.biomedcentral.com/info/authors%%
%% and include the section headings         %%
%% accordingly for your article type.       %%
%%                                          %%
%% See the Results and Discussion section   %%
%% for details on how to create sub-sections%%
%%                                          %%
%% use \cite{...} to cite references        %%
%%  \cite{koon} and                         %%
%%  \cite{oreg,khar,zvai,xjon,schn,pond}    %%
%%  \nocite{smith,marg,hunn,advi,koha,mouse}%%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%%%%%%%%%%%%%%%%%%%%%%%% start of article main body
% <put your article body there>

%%%%%%%%%%%%%%%%
%% Background %%
%%
\section*{Introduction}

Molecular docking tools are routinely utilised to predict the binding pose as well as the binding affinity of a ligand, usually a small organic molecule, when it is bound to a target protein of interest. The predicted pose helps to reveal the putative intermolecular interactions and therefore understand the mechanism of protein-ligand binding, whereas the predicted affinity reflects the binding strength and therefore prioritizes strong-binding ligands over weak-binding ones from a large library of compounds to evaluate.

A typical docking program implements a sampling algorithm to generate possible binding poses and a scoring function to estimate their binding affinity. The former operation is known as pose generation, and the latter is known as scoring. Modern docking tools such as AutoDock Vina \cite{595} and idock \cite{1362} are already capable of generating near-native poses with a redocking success rate of more than 50\% on three diverse benchmarks \cite{1362}.

Meanwhile, recent years have seen the emergence and prosperity of a new class of scoring functions that use machine learning techniques to increase the accuracy of binding affinity prediction, with RF-Score \cite{564} being the first machine-learning scoring function that introduced a significant improvement over classical, non-machine-learning scoring functions. Since then, numerous enhancements have been applied to the original RF-Score \cite{564} and thereby resulting in RF-Score-v2 \cite{1370} and RF-Score-v3 \cite{1647}, and other relevant studies \cite{1432}. RF-Score has been utilised \cite{1281} to discover a large number of innovative binders of antibacterial DHQase2 targets, demonstrating its practical utility. To promote its use, RF-Score-v3 has been incorporated into a popular web platform called istar \cite{1362}, available at http://istar.cse.cuhk.edu.hk/idock, for large-scale docking. A recent study \cite{1663} has highlighted the benefit of training machine-learning scoring functions with low-quality structural and interaction data.

In prospective virtual screening, accurate prediction of binding affinity of docked poses, rather than co-crystallised poses, is crucial for ranking compounds from a large molecular database. Nevertheless, to the best of our knowledge, all existing machine-learning scoring functions are trained on numerical features derived from crystal poses exclusively, without considering the impact of pose generation error. Hence their applicability to re-scoring docked poses remains unexamined, and the high accuracy vigorously claimed in their corresponding studies would thus potentially downgrade when their methods are applied to scoring docked poses.

Pose generation error is typically measured by comparing the geometry of the pose generated by the docking software and that of the same molecule co-crystallised with the considered protein (Figure \ref{fig:4WAF}). The impact of this error on binding affinity prediction is yet to be systematically analysed across diverse protein-ligand complexes. In this study we investigate the impact of pose generation error on the predictive performance of both classical and machine-learning scoring functions. We also study their capability of predicting the near-native pose that is conformationally close to the crystal pose.

\section*{Methods}

Preamble: Four models are purposely constructed for evaluation and comparison.

\subsection*{Model 1 - AutoDock Vina}

The AutoDock series \cite{597,596,595} is the most cited docking software by the research community, with over 8,000 citations to date between these three publications, according to Google Scholar. As a completely new counterpart of AutoDock 4 \cite{596}, AutoDock Vina \cite{595} substantially improved the average accuracy of the binding mode predictions, while running two orders of magnitude faster with multithreading. Vina was an exciting development, not only because of its remarkable pose generation performance in terms of both effectiveness and efficiency, but also because it is an open source tool and is among the most accurate classical scoring functions for binding affinity prediction.

Like all classical scoring functions, Vina assumes a predetermined functional form. In this case, Vina's score for the $k$th conformer, $e_k$, is calculated as:

\begin{equation}
\label{rfscore3:e_k}
e_k=\frac{e_{k,inter}+e_{k,intra}-e_{1,intra}}{1+w_6N_{rot}}
\end{equation}

Now because studies on binding affinity prediction are benchmarked on co-crystallised ligands to avoid confounding factors, there is only one conformer per molecule ($k=1$) and thus the intramolecular contribution cancels out giving:

\begin{equation}
\label{rfscore3:e_1}
e_1=\frac{e_{1,inter}}{1+w_6N_{rot}}
\end{equation}

where

\begin{eqnarray}
\label{rfscore3:e_1_inter}
e_{1,inter} &=& w_1 \cdot Gauss1_1 \nonumber \\
            &+& w_2 \cdot Gauss2_1 \nonumber \\
		    &+& w_3 \cdot Repulsion_1 \nonumber \\
		    &+& w_4 \cdot Hydrophobic_1 \nonumber \\
		    &+& w_5 \cdot HBonding_1
\end{eqnarray}

\begin{equation}
\label{rfscore3:w}
\mathbf w=(-0.035579,-0.005156,0.840245,-0.035069,-0.587439,0.05846)
\end{equation}

$e_1$ is the predicted free energy of binding reported by the Vina software when scoring the structure of a protein-ligand complex. The values for the six weights were found by Ordinary Least Squares (OLS) using a nonlinear optimisation algorithm as it has been the case in related force-field scoring functions \cite{1454}, although this process was not detailed in the original publication \cite{595}. The training data was PDBbind v2007 refined set (N=1300). $N_{rot}$ is the number of rotatable bonds. Unlike other classical scoring functions, Vina is not exactly a sum of energy terms because $w_6\neq0$, although it is quasi-linear since $1+w_6N_{rot}$ takes values close to 1 for most protein-ligand complexes. As usual, e.g. \cite{1362}, the predicted free energy of binding in kcal/mol units is converted into pKd with $pK_d=-0.73349480509e_1$ in order to compare to binding affinities ($pK_d$ or $pK_i$). Expressions and further details for the five $e_{k, inter}$ terms can be found in \cite{595,1362}.

\subsection*{Model 2 - MLR::Vina}

This is a multiple linear regression (MLR) model using the six unweighted Vina terms as features. The use of MLR as the regression model implies an additive functional form and hence MLR::Vina is a classical scoring function. It adopts the philosophy of empirical scoring functions.

In order to make the problem amenable to MLR, we made a grid search on the $w_6$ weight and thereafter ran MLR on the remaining five weights. Specifically, we sampled 101 values for $w_6$ from 0 to 1 with a step size of 0.01. Interesting we found that the $w_6$ values of the best models were always between 0.005 and 0.020. Then we again sampled 16 values for $w_6$ in this range with step size 0.001, and used the best of them in terms of the lowest RMSE (Root Mean Square Error) on the training set.

\subsection*{Model 3 - RF::Vina}

While Vina's ability to predict binding affinity is among the best provided by classical scoring functions, it is still limited by the assumption of a functional form. To investigate the impact of this modelling assumption, we used Random Forest (RF) \cite{1309} to implicitly learn the functional form from the data. Other machine learning techniques can of course be applied to this problem, e.g. SVR (Support Vector Regression) \cite{1295}, although this is out of the scope of the study.

A RF is an ensemble of many different decision trees randomly generated from the same training data \cite{1309}. RF trains its constituent trees using the CART algorithm \cite{1310}. Instead of using all features, RF selects the best data split at each node of the tree from a typically small number (mtry) of randomly chosen features. In regression problems, the RF prediction is given by arithmetic mean of all the individual tree predictions in the forest.

Here we built a RF model with the six Vina features using the default number of trees (500) and values of the mtry control parameter from 1 to all 6 features. The selected model was that with the mtry value providing the lowest RMSE on a subset of training data known as the OOB (Out of Bag) data. This process was repeated ten times with ten different random seeds because RF is stochastic. The predictive performance was reported for the RF with the best seed that led to the lowest RMSE on the test set. Further details on RF model building in this context can be found in \cite{564}.

\subsection*{Model 4 - RF::VinaElem}

This is the model described in the previous subsection with an expanded set of 42 features once the 36 RF-Score features are added to the six Vina features. For a given random seed, a RF for each mtry value from 1 to 42 was built and that with the lowest RMSE on OOB data was selected as the scoring function. Like in the training process of model 3, the same ten seeds were used, and the predictive performance was reported for the RF with the best seed that resulted in the lowest RMSE on the test set.

To calculate RF-Score features, atom types were selected so as to generate features that are as dense as possible, while considering all the heavy atoms commonly observed in PDB complexes (C, N, O, F, P, S, Cl, Br, I). As the number of protein-ligand contacts is constant for a particular complex, the more atom types are considered, the sparser the resulting features will be. Therefore, we selected a minimal set of atom types by considering atomic number only. Furthermore, a smaller set of interaction features has the additional advantage of leading to computationally faster scoring functions.

RF-Score features are defined as the occurrence count of intermolecular contacts between elemental atom types $i$ and $j$, as shown in equations \eqref{rfscore3:x_ij} and \eqref{rfscore3:x}, where $d_{kl}$ is the Euclidean distance between the $k$th protein atom of type $j$ and the $l$th ligand atom of type $i$ calculated from a structure; $K_j$ is the total number of protein atoms of type $j$ ($\#\{j\}=9$) and $L_i$ is the total number of ligand atoms of type $i$ ($\#\{i\}=4$) in the considered complex; $\mathcal{H}$ is the Heaviside step function that counts contacts within a $d_{cutoff}$ neighbourhood. For example, $x_{7,8}$ is the number of occurrences of protein oxygen atoms hypothetically interacting with ligand nitrogen atoms within a chosen neighbourhood. Full details on RF-Score features are available at \cite{564,1295}.

\begin{equation}
\label{rfscore3:x_ij}
x_{ij}=\sum_{k=1}^{K_j}\sum_{l=1}^{L_i}\mathcal{H}(d_{cutoff}-d_{kl})
\end{equation}

\begin{equation}
\label{rfscore3:x}
\mathbf x=\{x_{ij}\}\in N^{36}
\end{equation}

\subsection{Performance measures}

As usual \cite{1313}, performance will be measured by the Standard Deviation (SD), Root Mean Square Error (RMSE), Pearson correlation (Rp) and Spearman rank-correlation (Rs) between predicted and measured binding affinity. SD is included to permit comparison to previously-tested scoring functions on this benchmark. RMSE, on the other hand, reflects the ability of the scoring function to report an accurate binding affinity value. Rs shows how well it can rank bound ligands according to binding strength. Rp simply shows how linear the correlation is and thus it is a less relevant indicator of the quality of the prediction. The mathematical expressions of these four metrics can be found in \cite{1432}.

\begin{equation}
RMSE = \sqrt{\frac{1}{N}\sum_{n=1}^N(p^{(n)}-y^{(n)})^2}
\label{eqn:rmse}
\end{equation}

\begin{equation}
SD = \sqrt{\frac{1}{N-2}\sum_{n=1}^N(\hat{p}^{(n)}-y^{(n)})^2}
\label{eqn:sdev}
\end{equation}

\begin{equation}
R_p = \frac{N\sum_{n=1}^Np^{(n)}y^{(n)}-\sum_{n=1}^Np^{(n)}\sum_{n=1}^Ny^{(n)}}{\sqrt{(N\sum_{n=1}^N(p^{(n)})^2-(\sum_{n=1}^Np^{(n)})^2)(N\sum_{n=1}^N(y^{(n)})^2-(\sum_{n=1}^Ny^{(n)})^2)}}
\label{eqn:pcor}
\end{equation}

\begin{equation}
R_s = \frac{N\sum_{n=1}^Np_r^{(n)}y_r^{(n)}-\sum_{n=1}^Np_r^{(n)}\sum_{n=1}^Ny_r^{(n)}}{\sqrt{(N\sum_{n=1}^N(p_r^{(n)})^2-(\sum_{n=1}^Np_r^{(n)})^2)(N\sum_{n=1}^N(y_r^{(n)})^2-(\sum_{n=1}^Ny_r^{(n)})^2)}}
\label{eqn:scor}
\end{equation}

\section*{Results}

\subsection*{Redocking the ligand of each test set complex}

In this study, each of the 1300 co-crystallized ligands was redocked into the binding site of its target protein using Vina with default settings. Previously, a script was written to automatically define the search space by finding the smallest cubic box that covers the entire ligand and subsequently extending the box by 10Å in all the three dimensions. For each molecule, Vina returned a maximum number of nine docked poses, of which the one with the best Vina score was used. A second script was written to compute their RMSD with respect to the corresponding co-crystallized pose. Because we aimed at investigating the impact of pose generation error on the prediction of binding affinity, a second test set was defined where each of the 195 complexes has its ligand re-docked and its binding affinity predicted by the scoring functions previously trained on the 1105 crystal structures. As a baseline, these scoring functions were also tested on the co-crystallized ligands of the same 195 complexes. It is noteworthy that, in redocked poses, Vina achieved a relatively small pose generation error in the test set (52\% of the ligands had a docked pose with RMSD < 2\AA). [please explain how waters and ions are dealt with]

\subsection*{Pose generation error slightly worsens binding affinity prediction}

Models 2-4 were trained on the same 1105 complexes and tested on the 195 complexes in the 2007 core set (Vina is model 1 and was originally trained on all 1300 complexes, so we only tested it on the core set). While this training set is composed by crystal structures, there are two versions of the test set: the “crystal” test set with 195 crystal structures and the “docked” test set with 195 re-docked structures (one per complex, generated as explained in the previous subsection). All models were tested on both versions of the test set. [please specific which of the docking poses is being re-scored]

Models 3 and 4 are stochastic because they are based on RF (a randomized algorithm). Hence, to assess the variability in their response, the same set of 10 random seeds were used to generate 10 versions of the model (a different seed per training run). The performance of each model on each test set version, i.e. on co-crystallized poses and redocked poses of the same complexes, is summarized by the boxplots in Figure 2. This performance is measured as the Root Mean Square Error (RMSE), Pearson’s correlation coefficient (Rp) and the Spearman’s rank-correlation coefficient (Rs) between measured and predicted binding affinity.

Fig. 2. Performance of each scoring function on the PDBbind v2007 core set test set with co-crystallized ligands (left of each plot) and the same set of test complexes with the re-docked ligand with the lowest Vina score instead (right). Three performance measures are presented: RMSE (top), Rp (middle) and Rs (bottom). Co-crystallized and docked ligands are re-scored with Vina (black), MLR::Vina (red), RF::Vina (green) and RF::VinaElem (blue). [wondering if more compact plots possible, these take a lot of space…]

Results in Figure 2 show that pose generation error introduces a small degradation in the ability of models 2-4 to rank-order complexes according to predicted binding affinity in all scoring functions (this can be seen in all three plots). In contrast, Vina performed much better on docked poses in terms of RMSE. The latter is a curious result and we are unable to explain it with the information provided in the original paper [2]. On the other hand, it is remarkable that the best scoring function, RF::VinaElem still achieves such a high performance despite pose generation error (see Figure 3). Importantly, since model 3 use the same features as model 1 and a subset of its training set (Vina is trained on all 1300 complexes in PDBbind v2007 refined set, whereas model 3 trains on the 1105 left after removing the 195 complexes in the v2007 test set), RF::Vina performs remarkably better at predicting binding affinity than the widely-used Vina while having the same applicability domain.

\subsection*{Dependency of RMSD with binding affinity prediction}

Next, we compare the RMSD of the redocked pose with the individual absolute error in its binding affinity prediction by Vina and RF::VinaElem (note that the square root of the summation of the square of these errors is the RMSE introduced in section 3.2). It is widely believed that the higher the pose generation error the larger the error on predicting that pose will be. Figure 4 plots this information for each scoring function. Strikingly, both scoring functions are particularly robust to pose generation error, with accurate prediction still being obtained in poses with RMSDs of almost 15. This is likely to be connected to uncertainty associated to relating a static crystal structure of the complex with its measured pKd which is the outcome of the dynamic process of binding, as discussed by Ballester et al. [8] To the best of our knowledge, these behaviour has not been communicated yet for classical SFs, which is very surprising given that these have been around for more than 30 years. On the other hand, it is noteworthy that, while some complexes are very well predicted (pKd error ~ 0), some other have errors of more than 7 orders of magnitude (see left plot in Figure 4). However, the performance over all the test complexes remains high (see Figure 3). [by the way was was the role of e\_intra features? Compared to set 1 or results from LNBI-1 only small improvement, right?, state with which SFs are we using here e\_intra features]

Fig. 3. Performance on the 195 test set complexes in the PDBbind benchmark: AutoDock Vina (model 1; left) and RF::VinaElem (model 4; right). RF::VinaElem constitutes a remarkable improvement on the key requirement of predicting binding affinity when re-scoring redocked poses.

Fig. 4. RMSD from redocking the 195 test set complexes in the PDBbind benchmark: AutoDock Vina (model 1; left) and RF::VinaElem (model 4; right). The y-axis shows the absolute error in predicting pKd for each complex. These results show that both scoring functions are particularly robust to pose generation error. The Rp and Rs stated at the top of these plots quantifies how little the RMSD of the complex generally influences binding affinity prediction, at least when using these scoring functions (these correlations must not be confused with those between predicted and measured pKd in Figure 3).

\subsection*{Using the re-docked poses of training complexes corrects for pose generation error}

Until now training on crystal and testing on re-docked poses without changing composition of training or test sets. Here still testing on re-docked poses (the best pose of each molecule defined as that with the lowest free energy of binding according to Vina), but now training on re-docked poses which improves test set performance with respect to the SF trained on crystal. I think this is set 2 and you have all the figures, put the best one here. Important result: first time as far as I know.

\subsection*{Results from other sets}

Until now using one docking pose per molecule, multi-pose schemes 5 and 6 did not improve, but it is worth communicating that they didn’t work. We also propose another type of schemes, where the features of a molecule are calculated from the entire ensemble of poses (if Vina returns less than nine poses, then the feature vector for that molecule is completed by repeating the pose with lowest Vina score as many times as poses are missing). The same for the test set. For models 2-3 there will be 91 features per molecule then (10VinaTerms*9poses + 1 Nrot) [NB: check the READMEs in your code]

\section*{Discussion}

We should release RF-Score-v4: trained on docking poses with the latest PDBbind refined set.

Our results show that pose generation error affects the accuracy of scoring functions, which is well anticipated. To minimize this negative impact, re-training the scoring functions on docked poses instead of crystal poses can be a straightforward solution. On the other hand, we find that although machine-learning scoring functions are generally good at binding affinity prediction, they do not perform as well as classical scoring functions on native pose prediction. This indicates that predictions of binding affinity and native pose are two different tasks and no single scoring function performs optimally for both tasks.

Although we only used RF in this study, we believe our conclusions are also applicable to SVR (support vector regression)-based scoring functions \cite{1295,963} as well as other machine-learning scoring functions.

\section*{Conclusions}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                                          %%
%% Backmatter begins here                   %%
%%                                          %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\begin{backmatter}

\section*{Competing interests}
The authors declare that they have no competing interests.

\section*{Author's contributions}
P.J.B. designed the study. H.L. wrote the manuscript with P.J.B. H.L. implemented the software and ran all the numerical experiments. All authors discussed results and commented on the manuscript.

This work has been carried out thanks to the support of the A*MIDEX grant (n$^{\circ}$ ANR-11-IDEX-0001-02) funded by the French Government $\ll$Investissements d’Avenir`$\gg$ program, the Direct Grant from the Chinese University of Hong Kong and the GRF Grant (Project Reference 414413) from the Research Grants Council of Hong Kong SAR.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                  The Bibliography                       %%
%%                                                         %%
%%  Bmc_mathpys.bst  will be used to                       %%
%%  create a .BBL file for submission.                     %%
%%  After submission of the .TEX file,                     %%
%%  you will be prompted to submit your .BBL file.         %%
%%                                                         %%
%%                                                         %%
%%  Note that the displayed Bibliography will not          %%
%%  necessarily be rendered by Latex exactly as specified  %%
%%  in the online Instructions for Authors.                %%
%%                                                         %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

% if your bibliography is in bibtex format, use those commands:
\bibliographystyle{bmc-mathphys} % Style BST file
\bibliography{../refworks}      % Bibliography file (usually '*.bib' )

% or include bibliography directly:
% \begin{thebibliography}
% \bibitem{b1}
% \end{thebibliography}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Figures                       %%
%%                               %%
%% NB: this is for captions and  %%
%% Titles. All graphics must be  %%
%% submitted separately and NOT  %%
%% included in the Tex document  %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%
%% Do not use \listoffigures as most will included as separate files

\section*{Figures}

\begin{figure}[h]
%\includegraphics[natwidth=15in,natheight=20in,width=0.98\linewidth]{../rescoring2/4WAF-3K6.png}
\caption{\csentence{Example of pose generation error.} Top: crystal structure of PI3K$\alpha$ in complex of a tetrahydropyrazolo[1,5-a]pyrazine codenamed 3K6 (PDB ID: 4WAF). Bottom: re-docked pose of 3K6, generated by idock \cite{1362}. Hydrogen bonds are rendered as dashed cyan lines, and $\pi$ stackings are rendered as dashed pink lines. The RMSD (Root-Mean Square Deviation) between the co-crystallised pose and the re-docked pose is 1.15 \AA, which is a quantitative measure of pose generation error. These two plots were created by iview \cite{1366}, an interactive WebGL visualizer that circumvents the requirement of Java, yet supports the construction of macromolecular surface and the display of virtual reality effects and molecular interactions. iview is freely available at http://istar.cse.cuhk.edu.hk/iview/.
}
\label{fig:4WAF}
\end{figure}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Tables                        %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%% Use of \listoftables is discouraged.
%%
\section*{Tables}

\begin{table}[ht]
\caption{The statistics of the five partitions of PDBbind v2013 refined set (N=2959).}
\label{tbl:partitions}
\begin{tabular}{rrrr}
\hline
\# & complexes & lowest pKd & highest pKd\\
\hline
1 & 592 & 2.00 & 11.74\\
2 & 592 & 2.00 & 11.80\\
3 & 592 & 2.00 & 11.85\\
4 & 592 & 2.00 & 11.92\\
5 & 591 & 2.05 & 11.72\\
\hline
\end{tabular}
\end{table}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%                               %%
%% Additional Files              %%
%%                               %%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\section*{Additional Files}

%\subsection*{cv.csv}
%This CSV file contains the PDB IDs and measured binding affinities of the protein-ligand complexes in the five partitions of PDBbind v2013 refined set for cross validation purpose.

\end{backmatter}
\end{document}
