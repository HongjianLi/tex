\documentclass[a4paper,12pt]{article}
\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage[compact]{titlesec}
\usepackage[numbers,sort&compress]{natbib}
\usepackage{amsmath}
\linespread{1.0}

\begin{document}

%\title{Personalized Oncology Supported by Next-Generation Computer-Aided Drug Repositioning, Ensemble Docking and Deep Learning}
\title{A Next-­Generation Computer­-Aided Drug Discovery Platform for Anticancer Drug Repurposing Supported by Large-­Scale Cheminformatics Databases and Interdisciplinary Laboratory Validations}
\maketitle

\section*{Primary Field}

E2 2209 Bioinformatics

\section*{Secondary Field}

M2 1204 Cancer

\section*{Keywords}

Personalized medicine, anticancer therapy, drug repurposing, molecular docking, virtual screening, deep learning, interaction profiling

\section*{Abstract}

Developing a new drug could cost as much as US\$2.6B over 13.5 years. Both the industry and academia are intensively demanding robust computational methods to facilitate or even automate the process of modern drug discovery. This is termed computer-aided drug discovery (CADD).

CADD tools are sometimes used as a metaphor for medicinal chemists' bread and butter, reflecting their importance in R\&D. Unfortunately, many CADD tools are commercial, proprietary, not portable, or difficult to use. This reality has inevitably imposed a huge obstacle for R\&D productivity. Therefore in the past six years, we have been contributing to creating a free, open source, next-generation CADD toolset, aiming to substantially simplify the use of such tools while taking advantages of the latest methodological advancements.

To this end, our first attempt was the development of a heterogeneous platform named istar, which encapsulated high-throughput virtual screening, machine-learning binding affinity estimation, molecular interaction profiling and visualization into an automatic and unified pipeline, freely available at http://istar.cse.cuhk.edu.hk. Since it was launched in October 2013, istar has served 24583 web sessions to 11507 users from 2201 cities in 114 countries. These promising statistics demonstrate the utility of our previous work.

Our toolset was validated not only retrospectively on community benchmarks, but also prospectively in interdisciplinary real-world problems. Collaborating with an external team of clinical physicians, we have recently rediscovered seven drugs as anticancer agents, as they were shown to exhibit submicromolar inhibitory effects to colorectal, hepatocellular, and bladder carcinomas in vitro in assays and in vivo in mice. These newly identified medications of marketed drugs vigorously present an alternative and cheap clinical therapy for the treatment of cancers, hopefully saving the precious lives of millions of patients who cannot afford expensive imported drugs. We have filed patent applications for four of these novel medications.

Building upon our initial but successful work, in this project we propose to continue to enhance our in-house CADD platform in these key aspects: drug repositioning, ensemble docking, interaction profiling and molecular visualization. Briefly speaking, drug repositioning guarantees a fast and cheap route to new medications with a low attrition rate, as most preclinical optimizations and toxicity assessments are probably satisfied; ensemble docking would generate reliable binding poses as it takes into accounts structural variability of multiple protein conformations, and it also allows to focus on a patient-specific mutant subtype of the disease on a personalized basis; interaction profiling and molecular visualization permit to inspect and reveal the underlying binding mechanism through which the drug becomes effective, and also shed light on the cause of drug resistence at a fine-grained molecular level. We carefully choose these state-of-the-art technologies to study and improve because we firmly believe that they will, when properly integrated, constitute a powerful and unprecedented approach to modern drug discovery success.

Since the proposed approach is for general purposes, its medical applications are not limited to combating cancers, but could be expanded to treat other common diseases such as hyperuricemia, which we are already working on, provided that the structure of the protein of therapeutic interest is available, which is the only requirement of using our proposed platform. Fortunately, such macromolecular structural data are rapidly growing year by year. For popular proteins, their structures are usually experimentally solved. For rare proteins, their structures can be computationally modeled. In either case, the only prerequisite for user input is likely to be circumvented, thus we feel confident that the proposed system will be of wide use.

\section*{Long-term Impact}

This proposal is designed to significantly increase drug discovery success rate in a pragmatic and practical manner. Our proposed next-generation CADD platform will be able to reveal the fundamental mechanism that governs molecular binding, and suggest ways to strengthen or weaken such binding. With this essential knowledge, a considerably higher success rate can be anticipated when it comes to constructing novel compounds from molecular fragments, optimizing chemical scaffolds, improving target selectivity, minimizing off-target side effects, finding new indications of approved drugs, or even predicting synergistic outcome of drug combinations. Two notable long-term impacts will be that 1) more alternative and affordable drug therapies will become available for a broad range of patients who will otherwise have to opt for expensive treatments, and 2) personalized medicine will be realized which assists in making medical decisions, practices, interventions and products tailored to the individual patient based on their response or risk of disease by considering their mutant subtype of pharmacophoric proteins.

\section*{Project Objectives}

\begin{enumerate}
  \item To collect and curate compound data from multiple external sources, including approved, experimental and withdrawn drugs (not only in US, but also in Europe, UK, Canada, Japan), regulated chemicals, herbal isolates, traditional Chinese medicines, natural products, and easily synthesizable compounds.\label{objective:cdata}
  \item To develop and release a new version of molecular docking software with atomic contact parameters and scoring function terms tuned to more consistently produce low-energy conformations geometrically closer to the co-crystallized conformation.\label{objective:idock}
%  \item To debug and revise the protein-ligand interaction profiling algorithm currently implemented in our molecular visualizer named iview, and verify the profiler using a database of known and putative interactions in the structural human proteome.
%  \item To rewrite, modulize, accelerate and standardize iview by exploiting the new features of JavaScript's latest specification codenamed ECMAScript 6, for instance, asynchronous programming.
  \item To create a new web server to seamlessly consolidate the above-mentioned data sources, docking methods, profiling algorithms and visualization features in order to streamline automatic ensemble docking of various types of compounds on a personalized basis.\label{objective:edock}
  \item To understand and elaborate molecular binding mechanism and analyze binding patterns at a fine-grained atomic level, and to suggest directions to strengthen or weaken intermolecular interactions so as to maximize selectivity and minimize side effect.\label{objective:iview}
  \item To utilize the new web server in identifying novel inhibitors of selected oncoproteins of patient-specific subtypes to achieve personalized oncology.\label{objective:edockapp}
  \item To evaluate the efficacy and cytotoxicity of candidate anticancer compounds in vitro and in vivo, and file patent applications for those showing significant inhibitory activities.\label{objective:wetval}
\end{enumerate}

\section*{Background of Research}

Computer-aided drug discovery (CADD) has now been widely accepted as a cost- and time-efficient strategy alternative to purely biochemical approaches. Robust and reliable computational methods are highly demanded by the pharmaceutical industry in order to accelerate or automate the early phases of modern drug discovery prior to preclinical experiments.

Although improvements to CADD tools are constantly reported, from a practical and pragmatic standpoint we unfortunately observe that many of them are commercial and proprietary. Freeware is normally designed to tackle a specific challenge in a specific domain only. These software programs developed by seaprate groups often follow diverse standards, requiring different input formats and generating different output formats, which would lead to complicated and error-prone chaining operations if the task were to combine multiple tools in different areas of CADD to achieve one single complete objective. This is often the case in virtual screening, a classical and daunting problem in CADD.

Virtual screening refers to searching libraries of compounds to identify those which are likely bioactive against a selected drug target. Virtual screening can be methodologically categoried into ligand-based virtual screening (LBVS) and docking-based virtual screening (DBVS), with the difference being that DBVS requires a 3D structure of the target but LBVS does not. In this project we concentrate on DBVS only, though we are meanwhile intensively researching LBVS approaches \citep{1749}. A typical workflow of DBVS includes molecular docking of a library of compounds against a target protein, followed by estimation of binding affinity, optionally interaction profiling and visualization, and then selection of candidate compounds. These individual steps will be outlined in the following two sections: works done by others and works done by us.

\subsection*{Works done by others}

Protein-ligand docking is a computational method that predicts how a small molecule, termed ligand, binds to a target protein, as well as how strongly they bind. Hence docking is useful in elaborating intermolecular interactions and enhancing the potency and selectivity of binding. In the context of DBVS, once a target protein is identified, a database of compounds will be docked against the protein, and their binding strength will be evaluated too. This is to shortlist compounds that are likely to show the strongest binding towards proteins intended to be inhibited (maximizing efficacy), or compounds that are likely to show the weakest binding towards proteins intended not to be inhibited (minimizing side effect).

The AutoDock suite \citep{1730} is unquestionably the most cited free software for protein-ligand docking. In particular, AutoDock Vina \citep{595} is a competitive docking program not only because it is free and open source, but also because it has been shown to substantially improve the average accuracy of binding mode prediction and run faster by an order of magnitude than its counterpart AutoDock 4 \citep{596}. Since Vina was released in the second half of 2010, it has been cited more than 3,700 times in just six years! The great success of Vina has led to a plenty of subsequent tools derived from Vina but improved in certain aspects. To name some, QuickVina \citep{1193} and QuickVina 2 \citep{1664} introduced first-order-consistency-check heuristics to accelerate local conformational search; VinaMPI \citep{1329} exploited Message Passing Interface (MPI) to distribute computing tasks to cluster computers for parallel execution; \citeauthor{1716} \citep{1716} invented a new, improved hybrid scoring function through combining the energy terms from AutoDock and Vina; Vinardo \citep{1741} proposed a scheme to systematically tune the scoring function parameters and suggested new values for the weights of each term; PSOVina \citep{1789} implemented particle swarm optimization algorithm and reduced the execution time by 51-60\% without compromising the prediction accuracy.

Although docking has managed to produce reasonably good binding poses with a success rate of more than 50\% \citep{1362}, it has been reviewed \citep{1695} that the traditionally low accuracy of the scoring function is a critical limitating factor of precisely estimating the affinity upon binding. To address this limitation, numerous scoring functions have been proposed. Linear scoring functions (e.g. Cyscore \citep{1372} and WScore \citep{1736}) are defined by the assumption of a fixed functional form to relate the predicted binding affinity to the numerical features that characterize the protein-ligand complex. They often employ standard multivariate linear regression (MLR) on experimental data to calibrate the coefficients in a weighted sum of physically meaningful terms as an estimation of binding affinity. Recent years have seen a fast growing number of new developments of machine-learning scoring functions, with RF-Score \citep{564} being the first that introduced a large improvement over classical approaches. RF-Score employed random forest and its second version RF-Score-v2 \citep{1370} increased predictive performance by substituting more precise distance-dependent features. CScore \citep{1194}, B2Bscore \citep{1410} and SFCscoreRF \citep{1347} are other examples which are also based on random forest. Apart from random forest, other machine-learning algorithms have been applied too. SVR-KB and SVR-EP \citep{963}, ID-Score \citep{1305} and MD-SVR \citep{1452} are representative scoring functions based on support vector regression, whereas NNScore 2.0 \citep{977} is based on neural networks.

As a downstream procedure of docking, visualization plays a key role in elucidating intermolecular interactions at an atomic level and aiding candidate compound selection. Thanks to the popularity of web servers and services, a number of online molecular visualizers have arisen in the past few years. Compared with offline counterparts, web-based visualizers have the apparent advantage of allowing users to conveniently visualize and inspect 3D structures directly in a modern browser, saving the trivial effort of downloading and installing any prerequisite. JSmol \citep{1314} is a JavaScript rewrite of the tradional and defacto visualizer Jmol (http://www.jmol.org). GLmol (http://webglmol.sourceforge.jp), 3Dmol.js \citep{1652} and NGL Viewer \citep{1666} are prevalent WebGL molecular viewers exploiting hardware rendering.

Described above are solitary pieces of standalone tools which were designed to solve a specific problem of CADD. There are some attempts on the feasibility of full automation of protein-ligand docking, analysis and visualization. DOCK Blaster \citep{557} utilizes DOCK \citep{1222} as the docking engine and ZINC \citep{1178} as the compound database. iScreen \citep{899} utilizes PLANTS \citep{607,779} as the docking engine and TCM@Taiwan \citep{528} as the compound database.

\subsubsection*{Works done by us}

In the past six years, we have been constructing a next-generation CADD toolset, which features several key modules: 1) a molecular docking program for high-throughput DBVS \citep{1153}, 2) a software-as-a-service platform for general-purpose web applications \citep{1362}, 3) an interactive web visualizer tailored to DBVS \citep{1366,1265}, 4) two accurate scoring functions for predicting intermolecular binding affinity of crystal and docked poses \citep{1647,1796,1433,1795,1797,1434}, 5) a de novo drug design tool considering synthetic feasibility \citep{1409,1387}, and 6) a shape recognition web server for ultrahigh-throughput LBVS \citep{1749}. It is worthwhile to highlight that all our tools are free and open source, well maintained and recognized.

In 2012 we developed a protein-ligand docking program called idock \citep{1153}. The goal was to further improve the computational performance so as to screen larger databases of compounds within a reasonable time. To this end, we substantially revised the numerical approximation algorithm of atomic free energy calculation from linear interpolation to table loookup, and introduced a heuristic of disabling inactive torsions to reduce the dimension of variables in conformational optimization. Unlike Vina where the input protein structure has to be parsed and energy grid maps have to be populated every time it attempts to dock a single ligand, in idock these are just one-off tasks and the results will be cached and reused. Compared with Vina, idock obtained a speedup of 3.3x in terms of CPU time and a speedup of 7.5x in terms of elapsed time on average, making it particularly suitable for performing large-scale DBVS.

In 2013 we implemented a modern software-as-a-service (SaaS) platform called istar \citep{1362}, originally motivated by the desire to realize molecular data preparation, format conversion, DBVS, analysis and visualization in one go, considerably relaxing the requirements on user's knowledge and expertise. Unlike DOCK Blaster or iScreen which neither support selecting compounds to dock with physiochemical properties nor allowing monitoring job progress in real time, our istar supports both features and additionally enables post-docking analysis such as interaction profiling and visualization, permitting users to easily study and investigate the binding mode of candidate compounds. As a byproduct, a huge database of more than 23 million purchasable compounds covering a large space of chemical diversity was provided for screening. This is thus far the largest open source web server ever available for performing DBVS. To date, istar has served 24583 web sessions to 11507 users from 2201 cities in 114 countries worldwide, according to Google Analytics.

In 2014 we developed an interactive molecular visualizer called iview \citep{1366,1265} embracing the WebGL technology, thus exploiting hardware rendering and implementing easy accessibility and platform independence. The unique feature that distinguishes iview from all other visualizers is its inherent support for docking entity extraction, binding cavity identification and binding pose analysis, tailor-made for virtual screening, both DBVS and LBVS. In terms of DBVS functionalities, iview enables macromolecular surface generation ported from the EDTSurf algorithm \citep{1297} and binding interaction profiling ported from the BINANA algorithm \citep{1413}.

In addition to tool development, we have meanwhile conducted empirical research on the feasibility of using machine-learning algorithms to enhance scoring function precision. First, we investigated under what circumstances machine-learning scoring functions would outperform classical ones, and found that this is the case when there are sufficient numerical features and training samples \citep{1432}. Based on this finding, we improved RF-Score by incorporating six additional features derived from Vina and by inflating the training set to all available structures in the refined set of PDBbind \citep{1633}. This led to the release of RF-Score-v3 \citep{1647}, a new tool for accurately predicting the binding affinity of a crystal pose.

Next, we studied the impact of docking pose generation error on the accuracy of machine-learning scoring functions \citep{1795,1797,1434}, and proposed a procedure to correct a substantial part of this error which consists of calibrating the scoring functions with re-docked poses, rather than co-crystallised poses. As a result, test set performance after this error-correcting procedure was much closer to that of predicting the binding affinity in the absence of pose generation error. This led to the release of RF-Score-v4 \citep{1795}, a new tool offering accurate estimation of binding affinity of a docked pose.

Furthermore, we demonstrated for the first time that training with low-quality structural and interaction data can still improve predictive performance \citep{1663}, contrary to the widely-held belief that additional performance can only be gained from high-quality data. This novel finding gives a new direction to further increase predictive accuracy.

Most importantly, our toolset has been independently validated to be practically useful in tackling real-world challenges. Collaborating with a team of clinical physicians led by the Co-I, we prospectively applied our toolset and successfully repurposed seven approved or experimental drugs that exhibited submicromolar inhibitory effect to colorectal, hepatocellular or bladder carcinomas \textit{in vitro} and \textit{in vivo} in nude mice \citep{1667,1681}. The newly identified medications of marketed drugs, which have a history of safe human use, vigorously present an alternative but cheap clinical therapy for cancers, hopefully saving millions of lives as many patients cannot afford costly imported drugs. In another case study, jointly with a medicinal chemist, we adopted the same computational approach and managed to reposition another approved drug as an anticancer agent \textit{in vitro}. In the third case study, collaborating with another clinical physician, we expanded our toolset's applicability domain and targetted the disease of gout (aka hyperuricemia), and discovered another approved drug which effectively reduced the serum uric acid levels in hyperuricemic mice \textit{in vivo}. Taken together, these independently-conducted case studies have sufficiently proved the success and significance of our work. We have filed patent applications for four of the new drug indications and will soon submit one more.

\section*{Research Plan and Methodology}

Sitting in the central position of this proposal is our promise to create a next-generation CADD platform in order to provide a complete solution to DBVS instead of simply supplying isolated pieces of solitary tools for each individual task. Here we define the adjective next-generation to be widely applicable and methodolocally advanced. One big contribution and commitment is that we guarantee our software will be free and open source under a permissive license.

The following subsections describe our plan to collect and cleanse compound data, realize ensemble docking on a personalized basis, and prospectively applying the platform to interdisciplinary anticancer drug discovery.

\subsection*{Collection and curation of compound data from multiple sources}

Compound data collection is the very first step. There are a lot of external sources of compound data. Manipulating a wide spectrum of data could be challenging, so we set up selection criteria to ensure effectiveness. To achieve project objective \ref{objective:cdata}, we decide to choose databases that are a) free and publicly available, b) popular and reliable, and c) of potential use in diverse scenarios. Therefore, we will focus on the following data sources (Table \ref{table:cdata}): DrugBank \citep{1594}, which contains approved, experimental and withdrawn drugs; TCM@Taiwan \citep{528}, which contains traditional Chinese medicines; SCUBIDOO \citep{1682}, which contains computationally created chemical compounds optimized toward high likelihood of synthetic tractability. There are important reasons of choosing these data. As a matter of fact, among the 84 drug products introduced to market in 2013, new indications of existing drugs accounted for 20\%. It is well known that medicinal plants and natural products generally cause fewer side effects than western drugs, which implies a higher success rate and a shorter time to marketing. Easily synthesizable compounds represent novel structures that are largely unexplored, enabling unpatented and diverse compounds to be discovered.
%Plot statistics and histogram distributions of physiochemcal properties.

\begin{table}
\caption{External sources of compound data}
\label{table:cdata}
\begin{tabular}{l|l|r}
  \hline
  Database & Content & \# of compounds\\
  \hline
  DrugBank & Approved, experimental, withdrawn drugs & 8,232\\
  \hline
  TCM@Taiwan & Traditional Chinese medicines & 37,170\\
  \hline
  SCUBIDOO & Computationally created chemical compounds & 999,794\\
  \hline
\end{tabular}
\end{table}

After data collection, the next step would be data curation. Here a three-step curation procedure is proposed, consisting of deduplication, conformer generation, and format conversion. First, we will filter out compounds containing missing or incomplete information (e.g. without a SMILES string), and remove duplicate compounds by calculating and comparing their canonical SMILES representation. Next, we will use a recent protocol, namely ETKDG \citep{1697}, to generate low-energy 3D conformers for each compound. The ETKDG protocol is chosen because it includes experimental torsional-angle preferences and additional knowledge of flat aromatic rings and linear triple bonds into the original distance geometry algorithm, which, by combining a post-processing step, was found to offer the best results in accuracy and second best in terms of efficiency comparing to four other popular conformer generators \citep{1127}. Finally we will convert the generated conformers to appropriate file formats to feed to idock for DBVS, aggregate individual records to form a unified dataset, and create an index and an interface for database query. Precalculated physiochemical properties will be retained and organized as key-value pairs. This proposed curation procedure merely requires a compound ID and a SMILES string for each compound, and is thus universally applicable to all data sources. Note that the above steps will not be performed manually, which would be tedious and error-prone. Instead, we will write computer scripts to automatically download and cleanse data. Unlike many other works where multiple data sources are to be integrated maually and the external data are collected only once, our use of computer scripts enables automatic data synchronization whenever the external sources are updated. This ensures our data will always be current.

The above-proposed compound data organization scheme is easy to manipulate and extend, making including future data sources as easy as just running the scripts. Depending on different applications, we will later consider supplementing the database with more types of compounds from ZINC15 \citep{1688}, WITHDRAWN \citep{1718}, SWEETLEAD \citep{1511}, TTD \citep{1790}, TCMSP \citep{1375}, SANCDB \citep{1680}, PubChem \citep{1701} and ChEMBL \citep{1424}, further magnifying the significance of our work. This project is innovative in that different sources of compound data will be analyzed, which represents a new opportunity given the recent availability of compiled sets of such molecules. The collected databases can be meanwhile used in the context of LBVS \citep{1749}, though it is out of the scope of this proposal.

\subsection*{Calibration of scoring function to improve docking performance}

It is a widespread observation that scoring functions are either apt at predicting binding conformation (docking capability), or apt at predicting binding affinity (scoring capability) \citep{1411,1695}. Here our interest is to calibrate the scoring function of idock \citep{1153} to improve its docking capability by following the same procedure as how Vinardo \citep{1741} was developed through calibrating the scoring function of Vina \citep{595}. An interesting finding from Vinardo was that the correlation coefficient between experimental and calculated binding affinities of a set of 72 scoring functions with small variations in the number of terms, parameters, weights and atom radii was poorly correlated to the percentage of correctly docked structures with a Pearson correlation coefficient of merely 0.47, whereas the average root mean square error (RMSD) between experimental and predicted binding conformations of the same 72 scoring functions was much better correlated to the percentage of correctly docked structures with an increased Pearson correlation coefficient of 0.78. This means scoring functions that excel in binding affinity prediction are unlikely to excel in binding conformation prediction. Therefore to improve docking capability, a reliable way is to develop scoring functions that minimize the average RMSD rather than maximizing scoring capability. This is the exact reason why we wish to follow the protocol used in the development of Vinardo.

In idock, the conformation-dependent binding energy $e$ is calculated as a weighted sum of five terms over all pairs of atoms $i$ and $j$ that can move relative to one another (equations \eqref{eqn:e} and \eqref{eqn:eij}) excluding 1-4 interactions, i.e. atoms separated by three consecutive covalent bonds. The five terms (equations \eqref{eqn:Gauss1} to \eqref{eqn:HBonding}) are essentially functions of the surface distance $d_{ij}$ of the considered atom pair, which can be calculated from the interatomic distance $r_{ij}$ and the Van der Waals radii $R_{t_i}$ and $R_{t_j}$ (equation \eqref{eqn:dij}). The first three terms account for steric interaction, as the general shape of a typical Lennard-Jones interaction can be approximated with a combination of an attractive Gaussian function and a repulsive parabolic function, given that the Gaussian term is negative and the parabolic function is positive. The last two terms are simple piecewise linear functions assessing hydrophobic effect and hydrogen bonding, which will be included in computation only when the atom types of the considered atom pair satisfy the formation condition of such physiochemical effect.

\begin{equation}
\label{eqn:e}
e = \sum_{i < j} e_{ij}
\end{equation}
\begin{eqnarray}
\label{eqn:eij}
e_{ij} &=& w_1 * Gauss_1(d_{ij}) \nonumber \\
       &+& w_2 * Gauss_2(d_{ij}) \nonumber \\
       &+& w_3 * Repulsion(d_{ij}) \nonumber \\
       &+& w_4 * Hydrophobic(d_{ij}) \nonumber \\
       &+& w_5 * HBonding(d_{ij})
\end{eqnarray}
\begin{equation}
\label{eqn:Gauss1}
Gauss_1(d_{ij}) = e^{-((d_{ij} - \sigma_1) / s_1)^2}
\end{equation}
\begin{equation}
\label{eqn:Gauss2}
Gauss_2(d_{ij}) = e^{-((d_{ij} - \sigma_2) / s_2)^2}
\end{equation}
\begin{equation}
\label{eqn:Repulsion}
Repulsion(d_{ij}) =
\begin{cases}
d_{ij}^2 & \text{if } d_{ij} < 0\\
0 &\text{if } d_{ij} \geq 0
\end{cases}
\end{equation}
\begin{equation}
\label{eqn:Hydrophobic}
Hydrophobic(d_{ij}) =
\begin{cases}
1 & \text{if } d_{ij} \leq p_1\\
p_2 - d_{ij} & \text{if } p_1 < d_{ij} < p_2\\
0 & \text{if } d_{ij} \geq p_2\\
\end{cases}
\end{equation}
\begin{equation}
\label{eqn:HBonding}
HBonding(d_{ij}) =
\begin{cases}
1 & \text{if } d_{ij} \leq h_1\\
d_{ij} / h_1 & \text{if } h_1 < d_{ij} < 0\\
0 & \text{if } d_{ij} \geq 0\\
\end{cases}
\end{equation}
\begin{equation}
\label{eqn:dij}
d_{ij} = r_{ij} - (R_{t_i} + R_{t_j})
\end{equation}

Apparently, weights $w_1$, $w_2$, $w_3$, $w_4$, $w_5$, parameters $\sigma_1$, $\sigma_2$, $s_1$, $s_2$, $p_1$, $p_2$, $h_1$, and Van der Waals radii $R_{t_i}$, $R_{t_j}$ are all tunable. Even the number of terms can be adjusted, i.e. some terms could be excluded. Here we propose a calibration protocol which consists of generating a variety of trial scoring functions with small perturbations in the number of terms, parameters, weights and Van der Waals radii, systematically exploring the vast combinatorial possibilities. Then each trial scoring function will be evaluated in terms of the average RMSD they result in. A combination of scoring, minimization, and redocking on carefully curated training datasets would warrant the development of a simplified scoring function with optimum docking performance.

In terms of computational performance, docking can be remarkably accelerated by utilizing GPU chips, which are programmable parallel processores providing extremely high computational throughput and tremendous memory bandwidth compared to conventional CPU chips. The PI's department has recently installed 52 GPU cards of GeForce GTX TITAN X and 32 GPU cards of Tesla K20m, mounting up to 239,616 cores, delivering a computational throughput of 456 TFLOPS and a memory bandwidth of 24 TB/s in total. These powerful computing facilities lay the hardware foundation for us to develop GPU-accelerated version of idock.

% Host & Card & \#\\
% gpu5 & K20m & 8\\
% gpu6 & K20m & 8\\
% gpu7 & TITAN X & 8\\
% gpu8 & TITAN X & 8\\
% gpu9 & TITAN X & 8\\
% gpu10 & TITAN X & 4\\
% gpu11 & TITAN X & 4\\
% gpu12 & TITAN X & 4\\
% gpu13 & TITAN X & 4\\
% gpu14 & TITAN X & 4\\
% gpu15 & TITAN X & 4\\
% gpu16 & TITAN X & 4\\

%We will optimize the GPU performance by maximizing parallel execution, memory bandwidth, and instruction throughput. Maximizing parallel execution can be achieved by exposing as much data parallelism as possible and mapping the parallelism to the hardware as efficiently as possible. We will develop a novel parallelization optimization algorithm based on Genetic Parallel Programming. In idock, we have parallelized both the pre-calculation of grid maps and the Monte Carlo global optimization using our novel thread pool in order to thoroughly utilize multi-core CPU. We plan to port these two most computationally demanding functions to the GPU, map Monte Carlo tasks directly to CUDA threads, and use NVIDIA's occupancy calculator to carefully choose the execution configuration of each kernel launch so as to maintain a high GPU utilization.

%Besides, we can efficiently utilize the memory bandwidth by (i) minimizing CPU-to-GPU data transfers and (ii) optimizing the access patterns to global memory and shared memory in the GPU. Since CPU-to-GPU and GPU-to-CPU data transfer bandwidths are much lower than that of the internal GPU, we will maximize the amount of data stored in the GPU global memory. In idock, constant data such as structure of receptor, definition of search space, pre-calculation of scoring function, and configurations for the BFGS Quasi-Newton local optimizer will be kept in constant cache, and atomic energy grid maps, due to its huge size, will be kept in global memory, while temporary variables will be kept in per-thread registers.

%To maximize global memory bandwidth, we will carefully design the access patterns, e.g. maximum bandwidth is achieved when the 32 threads of a warp access adjacent 4-byte words in a 128B L1 cache line because a single coalesced transaction would be enough to service that memory access. In our new docking program, we will adopt this aligned and sequential access patterns and organize array of structures (AOS) into structure of arrays (SOA).

Now we turn our attention from improving docking capability to improving scoring capability, which we have intensively studied \citep{1432,1647,1796,1433,1795,1797,1434,1663}. We demonstrated that substituting random forest for linear regression substantially improved binding affinity prediction. We believe this conclusion can be generalized to other advanced machine-learning regression methods, so we will attempt to exploit deep neural networks in the field of CADD \citep{1810}. The recent availability of software libraries for deep learning (such as Google's TensorFlow) grants us the feasibility of further increasing the predictive accuracy of state-of-the-art scoring functions such as RF-Score-v3 \citep{1647} and RF-Score-v4 \citep{1795}. This approach is extensible, which means besides binding affinity, pharmacokinetic properties such as acid dissociation constant pKa and octanol/water partition coefficient logP can be similarly predicted.

%\subsection*{Interaction profiling and visualization}

%It helps to learn new binding patterns and preferences.

%interaction profiling: BINANA \citep{1413}, PLIP \citep{1665}, PDID \citep{1712}

%ES6 features, garbage collection, arrow methods, modules, asynchronous programming

\subsection*{Creation of a brand new web server to realize personalized medicine}

Building upon our initial work on istar \citep{1362} as a general-purpose software-as-a-service (SaaS) platform, we feel confident to create a brand new web server, tentatively named edock, to seamlessly encapsulate the above-described curated compound data and tuned docking engine. A major advantage of edock over existing DBVS web servers will be its inherant support for personalized medicine. Take oncology for example. It is well known that drug resistence is common in cancers, often caused by mutations of oncogenes and oncoproteins. A drug therapy would be ineffective if it cannot precisely target the particular mutant of the intended oncoprotein of the patient on a personalized basis. In view of this problem, edock will be designed to intelligently instruct the user to correctly choose the desired protein structure carrying patient-specific mutations as DBVS target in order to circumvent resistence. As a result, the predicted candidate ligands would have a high chance of being bioactive and sensitive to the particular patient.

Another substantial advantage of edock will be its inherant support for ensemble docking, which uses an ensemble of multiple probable conformations of the protein as docking targets (Figure ). It has been repeatedly reported that for protein systems with large motions of loops or domains, prediction of reasonable poses of the ligand would be difficult if a rigid model is assumed for the receptor \citep{1730}. In many cases, ensemble docking results in an improvement over using a single protein structure \citep{1128}. Two successful real-world cases of ensemble docking are the discovery of approved drugs adapalene and fluspirilene as anticancer agents by the PI and the Co-I \citep{1667,1681}.

A straightforward question arisen from ensemble docking is how to select appropriate protein structures and how many should be selected. There are multiple answers to this question. In a recent publication it is concluded that using \textit{holo} structures containing small ligands could give poorer performance than using \textit{apo} structures \citep{1704}. Sampling a variety of receptor conformations using molecular dynamics and then performing docking simulations on these snapshots has also shown success \citep{1730}. Even computationally modeled or predicted protein structures are of value for DBVS \citep{1322,1277}. Taken together these advices, here forms our suggested guideline: for popular proteins having a bunch of experimentally solved structures, pick those with large ligands bound, even though they might not be of the highest resolution; for rare proteins having few structures, use the snapshots sampled from molecular dynamics; for rare proteins with no experimental structures at all, use the predictions generated from homology modeling. Hence edock will be applicable in any case, except that a higher success rate can be anticipated in the first case given more available protein structures.

\subsection*{Prospective interdisciplinary applications in anticancer drug discovery}

Practically we will apply our tools and datasets to realizing personalized medicine, with the first attempt on personalized oncology.

PI3K, ALK, Bcr-Abl, MDM2
Influenza, we will target the tail-loop binding domain of NP, the PB1-binding domain of PA, and the cap-binding domain of PB2. we concentrate on discovering inhibitors of three influenza A proteins: NP, PA and PB2. These viral proteins are structurally related in that NP forms homo-oligomers and multiple copies of NP wrap around genomic RNA, along with a trimeric RNA-dependent RNA polymerase (RdRP) of subunits PA, PB1 and PB2 making up a ribonucleoprotein (RNP) complex.

Based on our previous successful experience in repurposing clinically approved drugs as anticancer agents, we will first collect as many structures of these oncogenic targets as possible from the PDB database \citep{537}. Good targets: CDK2/4/6, FGFR3, EGFR, PI3K, ALK, Bcr-Abl, XDH. Cyclin-dependent kinases (CDK) 2/4/6 have been widely acknowledged and thoroughly documented as key proteins regulating the cell cycle and hallmarks for cancers. Epidermal growth factor receptor (EGFR), fibroblast growth factor receptor (FGFR) 3, phosphoinositide 3-kinase (PI3K), and murine double minute 2 (MDM2) are other important therpeutic targets of cancers. For instances, CDK2/4/6: colorectal, hepatocellular cancers; FGFR3: bladder cancer; PI3K: lung cancer;

Use edock to automate the following steps: analyze the desired binding cavity with our iview \citep{1366}. We will then extract the protein entity, convert file formats, and invoke our idock \citep{1153} to perform ensemble docking, which has the advantage of guaranteeing a consistent binding strength on average over multiple structures of the same oncogenic target with structural variabilities. Next we will rank the compounds, visualize their predicted conformations using iview \citep{1366}, analyze their putative binding interactions. Incidentally, this whole computational pipeline can be automated via advanced scripting as soon as our next-generation CADD framework is developed.

survey their reported usages from literature, and shortlist candidates for wet verifications. Lastly we will conduct cell viability assays, cell apotosis assays, cell cycle assays, western blotting, clinical trials on animals, and finally on humans. We expect this project to be of great impact, as our findings could possibly save human lives in millions. Figure 2.

In the long run, beyond just cancers, we will utilize our toolset for computer-aided drug discovery in much wider areas, including but not limited to gout, hyperuricemia, influenza, herpes, HBV and HIV viruses.

\section*{Existing facilities and major equipment available for this research proposal}

Existing facilities and equipments are sufficient to complete the proposed project.

In the PI's affiliation, which is CUHK's department of computer science and engineering, there are high-performance computers.

In the Co-I's affiliation, which is KMU's biotechnology center, there are wet laboratories and devices, and cancer cell lines and animal models.

\section*{Release of Completion Report and Data Archive Possibilities and Public Access of Publications Resulting from Research Funded by the RGC}

We will release the brand new web server named edock to the public free of charge upon project completion.

%We will release the following datasets to the public upon project completion:
%1) Curated collections of ~3000 approved drugs in US, Europe, Canada and Japan, with molecular properties and purchasing information.
%2) 23 million compounds in 3D format, ready for large-scale docking-based virtual screening.
%3) 1.7 billion conformers of 23 million compounds in 3D format, ready for large-scale shape-based virtual screening.

\section*{Education Plan}

A senior postdoctoral fellow will be hired to lead the project under the supervision of the PI. Two research assistants will be hired to help conduct empirical and systematic research. Seminars and tutorials will be organized every year. Financial support will be provided for the researchers to attend prestigious conferences overseas.

Every year the PI supervises several groups of postgraduate and undergraduate students for their MSc and final year projects. Some of them will be guided to the field of CADD and educated to use and evaluate our toolset and platform. They will also be instructed to refine existing functionalities and possibly implement new features.

The Co-I will foster a PhD student to perform interdisciplinary laboratory experiments such as cytotoxicity assays, western blotting, and oral dosage of drugs in mice, so as to validate the utility of the proposed platform.

\newpage
\linespread{0.5}
\footnotesize
\bibliographystyle{unsrtnat}
\bibliography{../refworks}

%\includefigure

\end{document}
